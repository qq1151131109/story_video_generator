"""
æµ‹è¯•é…ç½®æ–‡ä»¶
æä¾›å…¨å±€fixtureså’Œæµ‹è¯•å·¥å…·
"""
import pytest
import asyncio
import json
import tempfile
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, Any, List, Optional
from datetime import datetime

# ç³»ç»Ÿå¯¼å…¥
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config_manager import ConfigManager
from utils.file_manager import FileManager
from utils.enhanced_logger import setup_enhanced_logging
from utils.result_types import Result


@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
def test_config():
    """æµ‹è¯•é…ç½®"""
    return {
        "general": {
            "output_dir": "test_output",
            "temp_dir": "test_output/temp",
            "log_level": "DEBUG",
            "max_concurrent_tasks": 2,
            "supported_languages": ["zh", "en", "es"],
            "default_language": "zh"
        },
        "llm": {
            "default": {
                "model": "openai/gpt-3.5-turbo",
                "api_base": "https://api.openai.com/v1",
                "api_key": "test-key",
                "strict_mode": False
            },
            "script_generation": {
                "temperature": 0.8,
                "max_tokens": 1000
            }
        },
        "media": {
            "enable_integrated_generation": False,  # æµ‹è¯•æ—¶ç¦ç”¨
            "image": {
                "primary_provider": "mock",
                "resolution": "512x512"
            },
            "audio": {
                "primary_provider": "mock",
                "voice_speed": 1.0
            }
        },
        "video": {
            "resolution": "720x1280",
            "fps": 30,
            "enable_subtitles": True
        },
        "logging": {
            "level": "DEBUG",
            "console_level": "INFO",
            "max_file_size_mb": 1,
            "backup_count": 1,
            "files": {
                "main": {"filename": "test.log", "enabled": True, "level": "DEBUG"},
                "errors": {"filename": "test_errors.log", "enabled": True, "level": "ERROR"}
            }
        }
    }


@pytest.fixture
def temp_dir():
    """ä¸´æ—¶ç›®å½•fixture"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield Path(temp_dir)


@pytest.fixture
def config_manager(test_config, temp_dir):
    """é…ç½®ç®¡ç†å™¨fixture"""
    with patch.object(ConfigManager, '_load_main_config') as mock_load:
        mock_load.return_value = test_config
        config = ConfigManager()
        config.config = test_config
        # è®¾ç½®æµ‹è¯•è¾“å‡ºç›®å½•
        config.config['general']['output_dir'] = str(temp_dir)
        yield config


@pytest.fixture
def file_manager(config_manager):
    """æ–‡ä»¶ç®¡ç†å™¨fixture"""
    output_dir = config_manager.get('general.output_dir', 'output')
    return FileManager(output_dir)


@pytest.fixture
def logger_manager(config_manager):
    """æ—¥å¿—ç®¡ç†å™¨fixture"""
    return setup_enhanced_logging(config_manager.config)


@pytest.fixture
def sample_themes():
    """æµ‹è¯•ä¸»é¢˜æ•°æ®"""
    return {
        "chinese": [
            "åº·ç†™å¤§å¸æ™ºæ“’é³Œæ‹œçš„æƒŠå¿ƒä¼ å¥‡",
            "ç§¦å§‹çš‡ç»Ÿä¸€å…­å›½çš„å†å²å£®ä¸¾",
            "å”ç„å®—å¼€å…ƒç››ä¸–çš„è¾‰ç…Œ"
        ],
        "english": [
            "The Fall of the Roman Empire",
            "Napoleon's Last Battle at Waterloo",
            "The Rise of Alexander the Great"
        ],
        "spanish": [
            "La conquista de AmÃ©rica",
            "El Cid Campeador",
            "La Reconquista espaÃ±ola"
        ],
        "edge_cases": [
            "",  # ç©ºä¸»é¢˜
            "a",  # å•å­—ç¬¦
            "è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸é•¿çš„ä¸»é¢˜" * 10,  # è¶…é•¿ä¸»é¢˜
            "ç‰¹æ®Šå­—ç¬¦!@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
            "ğŸ¬ğŸ“ğŸ­ğŸªğŸ¨",  # emoji
        ]
    }


@pytest.fixture
def mock_api_responses():
    """Mock APIå“åº”æ•°æ®"""
    return {
        "openai_chat_completion": {
            "id": "chatcmpl-test",
            "object": "chat.completion",
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": "è¿™æ˜¯ä¸€ä¸ªå…³äºåº·ç†™æ™ºæ“’é³Œæ‹œçš„ç²¾å½©å†å²æ•…äº‹..."
                },
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 100, "completion_tokens": 200}
        },
        "runninghub_image": {
            "success": True,
            "data": {
                "task_id": "test-task-123",
                "status": "completed",
                "result_url": "https://example.com/test-image.jpg"
            }
        },
        "minimax_audio": {
            "success": True,
            "data": {
                "audio_url": "https://example.com/test-audio.mp3",
                "duration": 10.5,
                "subtitles": [
                    {"start": 0.0, "end": 5.0, "text": "ä½ çŸ¥é“å—ï¼Ÿ"},
                    {"start": 5.0, "end": 10.0, "text": "åº·ç†™æ˜¯å¦‚ä½•æ™ºæ“’é³Œæ‹œçš„ï¼Ÿ"}
                ]
            }
        }
    }


@pytest.fixture
def mock_file_content():
    """Mockæ–‡ä»¶å†…å®¹"""
    return {
        "test_image.jpg": b"fake_image_data",
        "test_audio.mp3": b"fake_audio_data",
        "test_video.mp4": b"fake_video_data",
        "test_subtitle.srt": "1\n00:00:00,000 --> 00:00:05,000\nä½ çŸ¥é“å—ï¼Ÿ\n\n2\n00:00:05,000 --> 00:00:10,000\nåº·ç†™æ˜¯å¦‚ä½•æ™ºæ“’é³Œæ‹œçš„ï¼Ÿ"
    }


class MockAPIClient:
    """Mock APIå®¢æˆ·ç«¯"""
    
    def __init__(self, responses: Dict[str, Any]):
        self.responses = responses
        self.call_count = 0
        self.last_request = None
    
    async def chat_completions_create(self, **kwargs):
        """Mock OpenAIèŠå¤©å®Œæˆ"""
        self.call_count += 1
        self.last_request = kwargs
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # æ ¹æ®è¾“å…¥è¿”å›ä¸åŒå“åº”
        if "script" in str(kwargs.get('messages', '')).lower():
            return Mock(**self.responses['openai_chat_completion'])
        else:
            response = self.responses['openai_chat_completion'].copy()
            response['choices'][0]['message']['content'] = "Mock response"
            return Mock(**response)
    
    async def post_json(self, url: str, data: Dict[str, Any]):
        """Mock HTTP POSTè¯·æ±‚"""
        self.call_count += 1
        self.last_request = {"url": url, "data": data}
        
        await asyncio.sleep(0.1)
        
        if "runninghub" in url:
            return self.responses['runninghub_image']
        elif "minimax" in url:
            return self.responses['minimax_audio']
        else:
            return {"success": True, "data": {}}


@pytest.fixture
def mock_api_client(mock_api_responses):
    """Mock APIå®¢æˆ·ç«¯fixture"""
    return MockAPIClient(mock_api_responses)


@pytest.fixture
def mock_llm_client(mock_api_client):
    """Mock LLMå®¢æˆ·ç«¯"""
    with patch('utils.llm_client_manager.LLMClientManager') as mock_class:
        mock_instance = Mock()
        mock_instance.get_client = Mock(return_value=mock_api_client)
        mock_instance.call_llm_async = AsyncMock(
            return_value=Result.success("Mock LLM response")
        )
        mock_class.return_value = mock_instance
        yield mock_instance


@pytest.fixture  
def mock_file_operations(mock_file_content, temp_dir):
    """Mockæ–‡ä»¶æ“ä½œ"""
    files = {}
    
    def mock_download(url: str, filepath: Path) -> bool:
        """æ¨¡æ‹Ÿæ–‡ä»¶ä¸‹è½½"""
        filename = filepath.name
        if filename in mock_file_content:
            filepath.parent.mkdir(parents=True, exist_ok=True)
            filepath.write_bytes(mock_file_content[filename])
            files[str(filepath)] = mock_file_content[filename]
            return True
        return False
    
    def mock_exists(filepath: Path) -> bool:
        """æ¨¡æ‹Ÿæ–‡ä»¶å­˜åœ¨æ£€æŸ¥"""
        return str(filepath) in files or filepath.exists()
    
    with patch('utils.file_manager.download_file', side_effect=mock_download), \
         patch.object(Path, 'exists', side_effect=mock_exists):
        yield files


@pytest.fixture
def performance_tracker():
    """æ€§èƒ½è¿½è¸ªå™¨"""
    class PerformanceTracker:
        def __init__(self):
            self.metrics = {}
            self.start_times = {}
        
        def start(self, operation: str):
            self.start_times[operation] = datetime.now()
        
        def end(self, operation: str):
            if operation in self.start_times:
                duration = (datetime.now() - self.start_times[operation]).total_seconds()
                self.metrics[operation] = duration
                del self.start_times[operation]
                return duration
            return 0
        
        def get_metrics(self) -> Dict[str, float]:
            return self.metrics.copy()
    
    return PerformanceTracker()


# è·³è¿‡æ¡ä»¶fixtures
@pytest.fixture
def skip_if_no_api():
    """å¦‚æœæ²¡æœ‰APIå¯†é’¥åˆ™è·³è¿‡æµ‹è¯•"""
    import os
    if not os.getenv('OPENROUTER_API_KEY'):
        pytest.skip("No API key available")


@pytest.fixture
def skip_if_slow(request):
    """è·³è¿‡æ…¢é€Ÿæµ‹è¯•çš„æ ‡è®°"""
    if request.config.getoption("--fast-only"):
        if request.node.get_closest_marker('slow'):
            pytest.skip("Skipping slow test in fast mode")


# å‚æ•°åŒ–fixtures
@pytest.fixture(params=["zh", "en", "es"])
def language(request):
    """å‚æ•°åŒ–è¯­è¨€fixture"""
    return request.param


@pytest.fixture(params=["åº·ç†™å¤§å¸", "Napoleon", "El Cid"])
def theme_by_language(request, language):
    """æ ¹æ®è¯­è¨€å‚æ•°åŒ–ä¸»é¢˜"""
    themes = {
        "zh": "åº·ç†™å¤§å¸æ™ºæ“’é³Œæ‹œ",
        "en": "Napoleon's Waterloo", 
        "es": "El Cid Campeador"
    }
    return themes.get(language, themes["zh"])


# æ¸…ç†fixtures
@pytest.fixture(autouse=True)
def cleanup_temp_files(temp_dir):
    """è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    yield
    # æµ‹è¯•ç»“æŸåæ¸…ç†
    import shutil
    try:
        if temp_dir.exists():
            shutil.rmtree(temp_dir, ignore_errors=True)
    except Exception:
        pass  # å¿½ç•¥æ¸…ç†å¤±è´¥


# pytesté’©å­å‡½æ•°
def pytest_configure(config):
    """pytesté…ç½®é’©å­"""
    # æ·»åŠ è‡ªå®šä¹‰æ ‡è®°
    config.addinivalue_line("markers", "unit: Unit tests")
    config.addinivalue_line("markers", "integration: Integration tests")
    config.addinivalue_line("markers", "e2e: End-to-end tests")


def pytest_addoption(parser):
    """æ·»åŠ å‘½ä»¤è¡Œé€‰é¡¹"""
    parser.addoption(
        "--fast-only", action="store_true", default=False,
        help="åªè¿è¡Œå¿«é€Ÿæµ‹è¯•"
    )
    parser.addoption(
        "--api-tests", action="store_true", default=False,
        help="è¿è¡Œéœ€è¦APIå¯†é’¥çš„æµ‹è¯•"
    )
    parser.addoption(
        "--performance", action="store_true", default=False,
        help="è¿è¡Œæ€§èƒ½æµ‹è¯•"
    )


def pytest_collection_modifyitems(config, items):
    """ä¿®æ”¹æµ‹è¯•æ”¶é›†"""
    if config.getoption("--fast-only"):
        # è·³è¿‡æ…¢é€Ÿæµ‹è¯•
        skip_slow = pytest.mark.skip(reason="Skipping slow test in fast mode")
        for item in items:
            if "slow" in item.keywords:
                item.add_marker(skip_slow)
    
    if not config.getoption("--api-tests"):
        # è·³è¿‡APIæµ‹è¯•
        skip_api = pytest.mark.skip(reason="API tests disabled")
        for item in items:
            if "api" in item.keywords:
                item.add_marker(skip_api)


@pytest.fixture
def assert_performance():
    """æ€§èƒ½æ–­è¨€è¾…åŠ©å‡½æ•°"""
    def _assert_performance(actual_time: float, expected_max: float, operation: str):
        assert actual_time <= expected_max, (
            f"{operation} took {actual_time:.2f}s, "
            f"expected <= {expected_max:.2f}s"
        )
    return _assert_performance


@pytest.fixture 
def assert_memory():
    """å†…å­˜æ–­è¨€è¾…åŠ©å‡½æ•°"""
    def _assert_memory(max_mb: float, operation: str):
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        assert memory_mb <= max_mb, (
            f"{operation} used {memory_mb:.1f}MB memory, "
            f"expected <= {max_mb:.1f}MB"
        )
    return _assert_memory