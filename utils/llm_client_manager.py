"""
å¤šæä¾›å•†LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
æ”¯æŒè‡ªåŠ¨fallbackåˆ°å¤‡ç”¨æä¾›å•†
"""
import asyncio
import logging
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import openai

@dataclass
class LLMProvider:
    """LLMæä¾›å•†é…ç½®"""
    name: str
    api_key: str
    base_url: str
    models: Dict[str, str]  # task_type -> model_name mapping
    enabled: bool = True

class LLMClientManager:
    """
    å¤šæä¾›å•†LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
    
    æ”¯æŒåŠŸèƒ½ï¼š
    1. å¤šä¸ªAPIæä¾›å•†è‡ªåŠ¨åˆ‡æ¢
    2. å¤±è´¥é‡è¯•å’Œfallbackæœºåˆ¶
    3. åŸºäº.envæ–‡ä»¶çš„çµæ´»é…ç½®
    4. è´Ÿè½½å‡è¡¡å’Œæˆæœ¬ä¼˜åŒ–
    """
    
    def __init__(self, config_manager):
        self.config = config_manager
        self.logger = logging.getLogger('story_generator.llm_manager')
        
        # åˆå§‹åŒ–æä¾›å•†é…ç½®
        self.providers = self._load_providers()
        self.clients = {}
        
        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„å®¢æˆ·ç«¯
        self._initialize_clients()
        
        self.logger.info(f"LLM Client Manager initialized with {len(self.providers)} providers")
    
    def _load_providers(self) -> List[LLMProvider]:
        """ä»é…ç½®æ–‡ä»¶åŠ è½½LLMæä¾›å•†é…ç½®"""
        providers = []
        
        # OpenRouter - ä¸»åŠ›æä¾›å•†
        if os.getenv('OPENROUTER_API_KEY'):
            # ğŸ”§ ä»é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹è€Œä¸æ˜¯ç¡¬ç¼–ç 
            openrouter_models = {}
            for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']:
                llm_config = self.config.get(f'llm.{task}', {})
                openrouter_models[task] = llm_config.get('model', 'openai/gpt-5')
            
            providers.append(LLMProvider(
                name='openrouter',
                api_key=os.getenv('OPENROUTER_API_KEY'),
                base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1'),
                models=openrouter_models
            ))
        
        # GPTsAPI - å¤‡ç”¨æä¾›å•†1 (GPT-5ä»£ç†)
        providers.append(LLMProvider(
            name='gptsapi',
            api_key='sk-yLda99082cd843c55e0453e3e23d384715143c3db7bmrz63',
            base_url='https://api.gptsapi.net/v1',
            models={
                'script_generation': 'gpt-5',
                'theme_extraction': 'gpt-5',
                'scene_splitting': 'gpt-5',
                'image_prompt_generation': 'gpt-5',
                'character_analysis': 'gpt-5'
            }
        ))
        
        # DeepSeek - å¤‡ç”¨æä¾›å•†2
        if os.getenv('DEEPSEEK_API_KEY'):
            providers.append(LLMProvider(
                name='deepseek',
                api_key=os.getenv('DEEPSEEK_API_KEY'),
                base_url=os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com'),
                models={
                    'script_generation': os.getenv('DEEPSEEK_MODEL', 'deepseek-chat'),
                    'theme_extraction': os.getenv('DEEPSEEK_MODEL', 'deepseek-chat'),
                    'scene_splitting': os.getenv('DEEPSEEK_MODEL', 'deepseek-chat'),
                    'image_prompt_generation': os.getenv('DEEPSEEK_MODEL', 'deepseek-chat'),
                    'character_analysis': os.getenv('DEEPSEEK_MODEL', 'deepseek-chat')
                }
            ))
        
        # Qwen - å¤‡ç”¨æä¾›å•†3
        if os.getenv('QWEN_API_KEY'):
            providers.append(LLMProvider(
                name='qwen',
                api_key=os.getenv('QWEN_API_KEY'),
                base_url=os.getenv('QWEN_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1'),
                models={
                    'script_generation': os.getenv('QWEN_MODEL', 'qwen-turbo'),
                    'theme_extraction': os.getenv('QWEN_MODEL', 'qwen-turbo'),
                    'scene_splitting': os.getenv('QWEN_MODEL', 'qwen-turbo'),
                    'image_prompt_generation': os.getenv('QWEN_MODEL', 'qwen-turbo'),
                    'character_analysis': os.getenv('QWEN_MODEL', 'qwen-turbo')
                }
            ))
        
        return providers
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰æä¾›å•†çš„å®¢æˆ·ç«¯"""
        # ä»é…ç½®è·å–è¶…æ—¶å’Œé‡è¯•å‚æ•°
        timeout = self.config.get('general.api_timeout', 120)
        max_retries = self.config.get('general.api_max_retries', 3)
        
        for provider in self.providers:
            try:
                client = openai.AsyncOpenAI(
                    api_key=provider.api_key,
                    base_url=provider.base_url,
                    timeout=float(timeout),
                    max_retries=max_retries
                )
                self.clients[provider.name] = client
                self.logger.debug(f"Initialized client for provider: {provider.name} (timeout={timeout}s, retries={max_retries})")
            except Exception as e:
                self.logger.error(f"Failed to initialize client for {provider.name}: {e}")
    
    async def call_llm_with_fallback(self, 
                                   prompt: str, 
                                   task_type: str = 'script_generation',
                                   temperature: float = 0.8,
                                   max_tokens: int = 1024) -> Optional[str]:
        """
        ä½¿ç”¨fallbackæœºåˆ¶è°ƒç”¨LLM
        
        Args:
            prompt: æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            LLMå“åº”å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        last_error = None
        
        # ä»é…ç½®è·å–é‡è¯•å»¶è¿Ÿæ—¶é—´
        retry_delay = self.config.get('general.retry_delay', 2)
        
        for provider in self.providers:
            if not provider.enabled or provider.name not in self.clients:
                continue
                
            client = self.clients[provider.name]
            model = provider.models.get(task_type, provider.models.get('script_generation'))
            
            try:
                self.logger.debug(f"Trying provider {provider.name} with model {model}")
                
                # æ ¹æ®æä¾›å•†è°ƒæ•´å‚æ•°é™åˆ¶
                adjusted_max_tokens = max_tokens
                if provider.name == 'deepseek':
                    # DeepSeeké™åˆ¶max_tokensåœ¨[1, 8192]èŒƒå›´å†…
                    adjusted_max_tokens = min(max_tokens, 8192)
                elif provider.name == 'qwen':
                    # Qwenä¹Ÿæœ‰ç±»ä¼¼é™åˆ¶
                    adjusted_max_tokens = min(max_tokens, 8000)
                
                response = await client.chat.completions.create(
                    model=model,
                    messages=[{
                        "role": "user",
                        "content": prompt
                    }],
                    temperature=temperature,
                    max_tokens=adjusted_max_tokens
                )
                
                content = response.choices[0].message.content
                if content:
                    self.logger.info(f"âœ… LLM call successful with provider: {provider.name}")
                    return content
                    
            except Exception as e:
                last_error = e
                error_msg = str(e).lower()
                
                # æ ¹æ®é”™è¯¯ç±»å‹åˆ†ç±»å¤„ç†
                if 'timeout' in error_msg or 'timed out' in error_msg:
                    self.logger.warning(f"â° Provider {provider.name} timeout: {e}")
                    # è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾…è¾ƒé•¿æ—¶é—´åå°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†
                    await asyncio.sleep(retry_delay * 2)
                elif 'rate limit' in error_msg or 'too many requests' in error_msg:
                    self.logger.warning(f"ğŸš« Provider {provider.name} rate limited: {e}")
                    # é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    await asyncio.sleep(retry_delay * 3)
                elif 'connection' in error_msg or 'network' in error_msg:
                    self.logger.warning(f"ğŸŒ Provider {provider.name} connection error: {e}")
                    # ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…é…ç½®çš„å»¶è¿Ÿæ—¶é—´
                    await asyncio.sleep(retry_delay)
                else:
                    self.logger.warning(f"âŒ Provider {provider.name} failed: {e}")
                    # å…¶ä»–é”™è¯¯ï¼Œç¨ä½œç­‰å¾…é¿å…å¿«é€Ÿè¿ç»­å¤±è´¥
                    await asyncio.sleep(retry_delay * 0.5)
                
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        self.logger.error(f"All LLM providers failed. Last error: {last_error}")
        raise Exception(f"All LLM providers failed: {last_error}")
    
    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        return [p.name for p in self.providers if p.enabled and p.name in self.clients]
    
    def disable_provider(self, provider_name: str):
        """ç¦ç”¨æŒ‡å®šæä¾›å•†"""
        for provider in self.providers:
            if provider.name == provider_name:
                provider.enabled = False
                self.logger.info(f"Disabled provider: {provider_name}")
                break
    
    def enable_provider(self, provider_name: str):
        """å¯ç”¨æŒ‡å®šæä¾›å•†"""
        for provider in self.providers:
            if provider.name == provider_name:
                provider.enabled = True
                self.logger.info(f"Enabled provider: {provider_name}")
                break
    
    def get_provider_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æä¾›å•†çŠ¶æ€"""
        status = {}
        for provider in self.providers:
            status[provider.name] = {
                'enabled': provider.enabled,
                'has_client': provider.name in self.clients,
                'models': provider.models
            }
        return status