"""
åŸºäºLangChainçš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨ - æ”¯æŒGPT-5æ–°APIæ ¼å¼
ä¿æŒä¸åŸLLMClientManagerç›¸åŒçš„æ¥å£ï¼Œä½¿ç”¨LangChainåº•å±‚å®ç°
æ”¯æŒGPT-5çš„responses.create()å’Œä¼ ç»Ÿçš„chat.completions.create()ä¸¤ç§APIæ ¼å¼
"""
import asyncio
import logging
import os
import httpx
import json
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass

# LangChain imports
from langchain_core.language_models import BaseLLM, BaseLanguageModel
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.exceptions import OutputParserException

# å¯¼å…¥æ–°çš„ç»“æ„åŒ–è¾“å‡ºè§£æå™¨
from utils.robust_output_parser import EnhancedLLMClient, RobustStructuredOutputParser
from utils.structured_output_models import (
    SceneSplitOutput, ImagePromptOutput, CharacterAnalysisOutput, ScriptGenerationOutput
)

# å¯¼å…¥åŸå§‹ModelConfigä»¥ä¿æŒå…¼å®¹æ€§
from core.config_manager import ModelConfig

@dataclass
class LangChainProvider:
    """LangChain LLMæä¾›å•†é…ç½®"""
    name: str
    llm: BaseLanguageModel
    models: Dict[str, str]
    enabled: bool = True

class GPT5NewAPIClient:
    """
    GPT-5æ–°APIæ ¼å¼å®¢æˆ·ç«¯
    ä½¿ç”¨responses.create()ç«¯ç‚¹è€Œä¸æ˜¯chat.completions.create()
    """
    
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.logger = logging.getLogger('story_generator.gpt5_api')
    
    async def create_response(self, 
                            messages: List[Dict[str, str]], 
                            model: str = "openai/gpt-5",
                            temperature: float = 0.8,
                            max_tokens: int = 1024) -> str:
        """
        ä½¿ç”¨GPT-5æ–°APIæ ¼å¼è°ƒç”¨LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            str: å“åº”å†…å®¹
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://story-video-generator.local",
            "X-Title": "Story Video Generator"
        }
        
        # GPT-5æ–°APIæ ¼å¼çš„è¯·æ±‚ä½“
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # å°è¯•æ–°çš„responsesç«¯ç‚¹
                response = await client.post(
                    f"{self.base_url}/responses",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    response_text = response.text
                    self.logger.debug(f"Raw response: {response_text[:200]}...")
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                    if not response_text or response_text.strip() == "":
                        self.logger.warning("Empty response from GPT-5 new API endpoint")
                        raise Exception("Empty response from GPT-5 new API endpoint")
                    
                    try:
                        result = response.json()
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Failed to parse JSON response: {e}, response: {response_text[:100]}")
                        # å°è¯•ä¼ ç»Ÿç«¯ç‚¹
                        return await self._fallback_to_legacy_api(messages, model, temperature, max_tokens, headers)
                    
                    # æ–°APIæ ¼å¼çš„å“åº”ç»“æ„
                    if "response" in result and "content" in result["response"]:
                        content = result["response"]["content"]
                        self.logger.info(f"âœ… GPT-5 new API call successful")
                        return content
                    elif "choices" in result and len(result["choices"]) > 0:
                        # å…¼å®¹ä¼ ç»Ÿå“åº”æ ¼å¼
                        content = result["choices"][0]["message"]["content"]
                        self.logger.info(f"âœ… GPT-5 new API call successful (legacy format)")
                        return content
                    else:
                        self.logger.warning(f"Unexpected response format: {result}")
                        return str(result)
                elif response.status_code == 404:
                    # æ–°ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œå°è¯•ä¼ ç»Ÿç«¯ç‚¹
                    self.logger.info("GPT-5 new API endpoint not found, falling back to legacy format")
                    return await self._fallback_to_legacy_api(messages, model, temperature, max_tokens, headers)
                else:
                    self.logger.error(f"GPT-5 API error {response.status_code}: {response.text}")
                    raise Exception(f"GPT-5 API error: {response.status_code}")
                    
        except httpx.TimeoutException:
            self.logger.error("GPT-5 API call timeout")
            raise Exception("GPT-5 API call timeout")
        except Exception as e:
            self.logger.error(f"GPT-5 API call failed: {e}")
            raise
    
    async def _fallback_to_legacy_api(self, 
                                    messages: List[Dict[str, str]], 
                                    model: str,
                                    temperature: float,
                                    max_tokens: int,
                                    headers: Dict[str, str]) -> str:
        """
        å›é€€åˆ°ä¼ ç»Ÿchat.completionsç«¯ç‚¹
        """
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    self.logger.info(f"âœ… GPT-5 legacy API call successful")
                    return content
                else:
                    raise Exception(f"Unexpected legacy response format: {result}")
            else:
                self.logger.error(f"GPT-5 legacy API error {response.status_code}: {response.text}")
                raise Exception(f"GPT-5 legacy API error: {response.status_code}")

class LangChainLLMManager:
    """
    åŸºäºLangChainçš„å¤šæä¾›å•†LLMç®¡ç†å™¨
    
    ä¼˜åŠ¿ï¼š
    1. ä½¿ç”¨LangChainçš„é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶
    2. å†…ç½®JSONè§£æå’ŒéªŒè¯
    3. æ›´å¥½çš„Promptç®¡ç†
    4. ä¿æŒä¸åŸæ¥å£100%å…¼å®¹
    """
    
    def __init__(self, config_manager):
        self.config = config_manager
        self.logger = logging.getLogger('story_generator.langchain_llm')
        
        # åˆå§‹åŒ–LangChainæä¾›å•†
        self.providers = self._initialize_langchain_providers()
        
        # ä¿ç•™GPT-5æ–°APIå®¢æˆ·ç«¯ä»¥å¤‡å°†æ¥ä½¿ç”¨
        self.gpt5_client = None
        if os.getenv('OPENROUTER_API_KEY'):
            self.gpt5_client = GPT5NewAPIClient(
                api_key=os.getenv('OPENROUTER_API_KEY'),
                base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')
            )
        
        # åˆå§‹åŒ–è¾“å‡ºè§£æå™¨
        self.json_parser = JsonOutputParser()
        
        # åˆå§‹åŒ–å¢å¼ºå®¢æˆ·ç«¯
        self.enhanced_clients = {}
        self._initialize_enhanced_clients()
        
        gpt5_status = "enabled" if self.gpt5_client else "disabled"
        self.logger.info(f"LangChain LLM Manager initialized with {len(self.providers)} providers, Enhanced parsers: enabled, GPT-5 API: {gpt5_status}")
    
    def _initialize_langchain_providers(self) -> List[LangChainProvider]:
        """åˆå§‹åŒ–LangChainæä¾›å•†ï¼Œä¼˜å…ˆçº§ï¼šOpenRouter(Gemini) > GPTsAPI(GPT-5) > DeepSeek"""
        providers = []
        
        # OpenRouteræä¾›å•†ï¼ˆé¦–é€‰ï¼Œä½¿ç”¨Geminiï¼‰
        if os.getenv('OPENROUTER_API_KEY'):
            openrouter_llm = ChatOpenAI(
                api_key=os.getenv('OPENROUTER_API_KEY'),
                base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1'),
                model="google/gemini-2.5-flash",  # ä¸»è¦æ¨¡å‹
                temperature=0.8,
                max_tokens=8192,
                timeout=30,
                max_retries=3
            )
            
            providers.append(LangChainProvider(
                name='openrouter',
                llm=openrouter_llm,
                models={task: 'google/gemini-2.5-flash' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
            ))
        
        # GPTsAPIæä¾›å•†ï¼ˆfallback GPT-5æä¾›å•†ï¼‰
        gptsapi_api_key = os.getenv('GPTSAPI_API_KEY', '')
        gptsapi_llm = ChatOpenAI(
            api_key=gptsapi_api_key,
            base_url='https://api.gptsapi.net/v1',
            model="gpt-5",  # GPTsAPIä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹åç§°
            temperature=0.8,
            max_tokens=8192,
            timeout=30,
            max_retries=3
        )
        
        providers.append(LangChainProvider(
            name='gptsapi',
            llm=gptsapi_llm,
            models={task: 'gpt-5' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
        ))
        
        # DeepSeekæä¾›å•†ï¼ˆå¤‡é€‰ï¼‰
        if os.getenv('DEEPSEEK_API_KEY'):
            deepseek_llm = ChatOpenAI(
                api_key=os.getenv('DEEPSEEK_API_KEY'),
                base_url=os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com'),
                model="deepseek-chat",
                temperature=0.8,
                max_tokens=8192,
                timeout=30,
                max_retries=3
            )
            
            providers.append(LangChainProvider(
                name='deepseek',
                llm=deepseek_llm,
                models={task: 'deepseek-chat' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
            ))
        
        return providers
    
    async def call_llm_async(self, 
                           messages: List[Dict[str, str]], 
                           config: ModelConfig,
                           task_type: str = "general",
                           expect_json: bool = False) -> str:
        """
        å¼‚æ­¥è°ƒç”¨LLM - ä¿æŒä¸åŸæ¥å£å…¼å®¹ï¼Œæ”¯æŒGPT-5æ–°API
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            config: æ¨¡å‹é…ç½®
            task_type: ä»»åŠ¡ç±»å‹
            expect_json: æ˜¯å¦æœŸæœ›JSONå“åº”
            
        Returns:
            str: LLMå“åº”å†…å®¹
        """
        
        # GPT-5æ–°APIæš‚æ—¶ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨LangChainæä¾›å•†
        # OpenRouterçš„/responsesç«¯ç‚¹è¿˜æœªå®ç°ï¼ŒGPTsAPIå·²æä¾›GPT-5æ”¯æŒ
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLangChainæ ¼å¼ï¼ˆç”¨äºä¼ ç»Ÿæä¾›å•†ï¼‰
        lc_messages = []
        for msg in messages:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            else:
                lc_messages.append(HumanMessage(content=msg["content"]))
        
        # å°è¯•LangChainæä¾›å•†
        last_error = None
        for provider in self.providers:
            if not provider.enabled:
                continue
                
            try:
                self.logger.debug(f"Trying provider {provider.name} with model {config.name}")
                
                # åŠ¨æ€æ›´æ–°æ¨¡å‹é…ç½®
                # æ ¹æ®æä¾›å•†é€‰æ‹©åˆé€‚çš„æ¨¡å‹
                if provider.name == 'deepseek':
                    model_to_use = 'deepseek-chat'
                elif provider.name == 'gptsapi':
                    # GPTsAPIä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹åç§°
                    if config.name == 'openai/gpt-5':
                        model_to_use = 'gpt-5'
                    else:
                        model_to_use = config.name
                elif provider.name == 'openrouter':
                    # OpenRouterä½œä¸ºfallbackï¼Œä½¿ç”¨Gemini
                    model_to_use = 'google/gemini-2.5-flash'
                else:
                    model_to_use = config.name
                
                provider.llm.model_name = model_to_use if hasattr(provider.llm, 'model_name') else model_to_use
                provider.llm.temperature = config.temperature
                provider.llm.max_tokens = config.max_tokens
                
                # è°ƒç”¨LLM
                if expect_json:
                    # å…ˆè·å–åŸå§‹å“åº”
                    result = await provider.llm.ainvoke(lc_messages)
                    response_text = result.content if hasattr(result, 'content') else str(result)
                    
                    # æ‰‹åŠ¨æ¸…ç†å’Œæå–JSON
                    try:
                        cleaned_json = self._clean_and_extract_json(response_text)
                        if cleaned_json:
                            # éªŒè¯JSONæ ¼å¼
                            parsed = json.loads(cleaned_json)
                            formatted_json = json.dumps(parsed, ensure_ascii=False, indent=2)
                            self.logger.debug(f"Successfully parsed and formatted JSON response")
                            return formatted_json
                        else:
                            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                            self.logger.error(f"Could not extract valid JSON from response")
                            self.logger.error(f"Response preview (first 500 chars): {repr(response_text[:500])}")
                            
                            # å°è¯•ä½œä¸ºæ™®é€šæ–‡æœ¬è¿”å›ï¼Œè®©ä¸Šå±‚å¤„ç†
                            if response_text.strip():
                                self.logger.info("Returning raw response text for manual processing")
                                return response_text
                            else:
                                raise Exception("Empty or invalid response from LLM")
                                
                    except json.JSONDecodeError as e:
                        self.logger.error(f"JSON parsing failed: {e}")
                        self.logger.error(f"Error location: line {getattr(e, 'lineno', 'unknown')}, column {getattr(e, 'colno', 'unknown')}")
                        self.logger.error(f"Error message: {getattr(e, 'msg', 'unknown')}")
                        
                        # å°è¯•ä¿®å¤å¸¸è§JSONé”™è¯¯
                        if cleaned_json:
                            fixed_json = self._attempt_json_repair(cleaned_json)
                            if fixed_json:
                                try:
                                    parsed = json.loads(fixed_json)
                                    self.logger.info("Successfully repaired JSON format")
                                    return json.dumps(parsed, ensure_ascii=False, indent=2)
                                except json.JSONDecodeError:
                                    self.logger.warning("JSON repair attempt failed")
                        
                        # è¿”å›åŸå§‹å“åº”
                        return response_text
                else:
                    # æ™®é€šæ–‡æœ¬å“åº”
                    result = await provider.llm.ainvoke(lc_messages)
                    response_text = result.content if hasattr(result, 'content') else str(result)
                    
                    self.logger.debug(f"Raw response from {provider.name}: {repr(response_text)[:200]}...")
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                    if not response_text or response_text.strip() == "":
                        self.logger.warning(f"Empty response from provider: {provider.name}")
                        raise Exception(f"Empty response from provider: {provider.name}")
                    
                    self.logger.info(f"âœ… LLM call successful with provider: {provider.name}")
                    return response_text
                    
            except Exception as e:
                self.logger.warning(f"ğŸŒ Provider {provider.name} failed: {e}")
                last_error = e
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        error_msg = f"All LLM providers failed. Last error: {last_error}"
        self.logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    def call_llm(self, messages: List[Dict[str, str]], config: ModelConfig, **kwargs) -> str:
        """åŒæ­¥è°ƒç”¨LLM - ä¿æŒä¸åŸæ¥å£å…¼å®¹"""
        return asyncio.run(self.call_llm_async(messages, config, **kwargs))
    
    def create_chain_for_task(self, task_type: str, prompt_template: str = None) -> Any:
        """
        ä¸ºç‰¹å®šä»»åŠ¡åˆ›å»ºLangChain Chain
        è¿™æ˜¯LangChainç‰¹æœ‰çš„åŠŸèƒ½ï¼Œå¯é€‰ä½¿ç”¨
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            prompt_template: æç¤ºè¯æ¨¡æ¿
            
        Returns:
            LangChain Chainå¯¹è±¡
        """
        if not self.providers:
            raise RuntimeError("No providers available")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æä¾›å•†
        provider = self.providers[0]
        
        if prompt_template:
            # åˆ›å»ºå¸¦æç¤ºè¯æ¨¡æ¿çš„Chain
            prompt = ChatPromptTemplate.from_template(prompt_template)
            chain = prompt | provider.llm
        else:
            # ç®€å•Chain
            chain = provider.llm
        
        self.logger.info(f"Created chain for task: {task_type}")
        return chain
    
    def _clean_and_extract_json(self, response_text: str) -> Optional[str]:
        """æ¸…ç†å“åº”æ–‡æœ¬å¹¶æå–JSON"""
        import re
        import json
        
        self.logger.debug(f"Original response preview: {repr(response_text[:200])}...")
        
        # æ›´å¼ºåŠ›çš„æ§åˆ¶å­—ç¬¦æ¸…ç†
        # ç§»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦ï¼ŒåŒ…æ‹¬æ¢è¡Œç¬¦ä»¥å¤–çš„æ‰€æœ‰æ§åˆ¶å­—ç¬¦
        cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', response_text)
        
        # è¿›ä¸€æ­¥æ¸…ç†ç‰¹æ®ŠUnicodeå­—ç¬¦å’Œä¸å¯è§å­—ç¬¦
        cleaned = re.sub(r'[\u200b-\u200f\u2028-\u202f\u2060-\u206f]', '', cleaned)
        
        self.logger.debug(f"Cleaned response preview: {repr(cleaned[:200])}...")
        
        # æ–¹æ³•1: æŸ¥æ‰¾```json...```æ ¼å¼
        json_match = re.search(r'```json\s*\n?(.*?)\n?```', cleaned, re.DOTALL | re.IGNORECASE)
        if json_match:
            content = json_match.group(1).strip()
            try:
                parsed = json.loads(content)
                self.logger.debug(f"Method 1 success: Found valid JSON in ```json``` block")
                return content
            except json.JSONDecodeError as e:
                self.logger.debug(f"Method 1 failed: {e}")
                pass
        
        # æ–¹æ³•2: æŸ¥æ‰¾```...```æ ¼å¼  
        code_match = re.search(r'```\s*\n?(.*?)\n?```', cleaned, re.DOTALL)
        if code_match:
            content = code_match.group(1).strip()
            if content.startswith('{') or content.startswith('['):
                try:
                    parsed = json.loads(content)
                    self.logger.debug(f"Method 2 success: Found valid JSON in ``` block")
                    return content
                except json.JSONDecodeError as e:
                    self.logger.debug(f"Method 2 failed: {e}")
                    pass
        
        # æ–¹æ³•3: é€’å½’æŸ¥æ‰¾åµŒå¥—JSONå¯¹è±¡
        def find_json_objects(text, start=0):
            """é€’å½’æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„JSONå¯¹è±¡"""
            objects = []
            
            # æŸ¥æ‰¾æ•°ç»„
            try:
                array_pattern = r'\[\s*\{.*?\}\s*(?:,\s*\{.*?\}\s*)*\]'
                for match in re.finditer(array_pattern, text[start:], re.DOTALL):
                    try:
                        parsed = json.loads(match.group())
                        objects.append(match.group())
                        self.logger.debug(f"Found valid JSON array: {len(match.group())} chars")
                    except json.JSONDecodeError:
                        continue
            except:
                pass
                
            # æŸ¥æ‰¾å¯¹è±¡
            brace_count = 0
            start_pos = -1
            
            for i, char in enumerate(text[start:], start):
                if char == '{':
                    if brace_count == 0:
                        start_pos = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_pos >= 0:
                        candidate = text[start_pos:i+1]
                        try:
                            parsed = json.loads(candidate)
                            objects.append(candidate)
                            self.logger.debug(f"Found valid JSON object: {len(candidate)} chars")
                        except json.JSONDecodeError:
                            continue
            
            return objects
        
        json_objects = find_json_objects(cleaned)
        
        # ä¼˜å…ˆè¿”å›åŒ…å«å¸¸è§å­—æ®µçš„JSON
        priority_fields = ['scenes', 'characters', 'image_prompt', 'video_prompt', 'content', 'text']
        
        for obj_str in json_objects:
            try:
                parsed = json.loads(obj_str)
                if isinstance(parsed, (dict, list)):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜å…ˆå­—æ®µ
                    if isinstance(parsed, dict):
                        for field in priority_fields:
                            if field in parsed:
                                self.logger.debug(f"Method 3 success: Found JSON with '{field}' field")
                                return obj_str
                    elif isinstance(parsed, list) and len(parsed) > 0:
                        if isinstance(parsed[0], dict):
                            self.logger.debug(f"Method 3 success: Found JSON array with {len(parsed)} items")
                            return obj_str
                    
                    # å¦‚æœæ²¡æœ‰ä¼˜å…ˆå­—æ®µï¼Œä½†æ˜¯æœ‰æ•ˆçš„JSONï¼Œä¹Ÿè¿”å›
                    self.logger.debug(f"Method 3 fallback: Found valid JSON without priority fields")
                    return obj_str
            except json.JSONDecodeError:
                continue
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        self.logger.warning(f"Failed to extract valid JSON from response. Original length: {len(response_text)}, Cleaned length: {len(cleaned)}")
        self.logger.debug(f"Cleaned response full text: {repr(cleaned)}")
        
        return None
    
    def _attempt_json_repair(self, json_str: str) -> Optional[str]:
        """å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜"""
        import re
        
        if not json_str or not json_str.strip():
            return None
        
        # ä¿®å¤å°è¯•åˆ—è¡¨
        repair_attempts = [
            # 1. ç§»é™¤æœ«å°¾å¤šä½™çš„é€—å·
            lambda s: re.sub(r',(\s*[}\]])', r'\1', s),
            
            # 2. ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·
            lambda s: re.sub(r"'([^']*)':", r'"\1":', s),
            
            # 3. ä¿®å¤ä¸å¸¦å¼•å·çš„é”®å
            lambda s: re.sub(r'(\w+):', r'"\1":', s),
            
            # 4. ç§»é™¤JSONå¤–çš„æ–‡æœ¬
            lambda s: self._extract_core_json(s),
            
            # 5. ä¿®å¤è½¬ä¹‰å­—ç¬¦é—®é¢˜
            lambda s: s.replace('\\"', '"').replace("\\'", "'"),
            
            # 6. ä¿®å¤ç¼ºå¤±çš„å¼•å·
            lambda s: re.sub(r':\s*([^",\[\]{}]+)([,}])', r': "\1"\2', s)
        ]
        
        current = json_str.strip()
        
        for i, repair_func in enumerate(repair_attempts):
            try:
                repaired = repair_func(current)
                if repaired and repaired != current:
                    # æµ‹è¯•ä¿®å¤åçš„JSONæ˜¯å¦æœ‰æ•ˆ
                    import json
                    json.loads(repaired)
                    self.logger.debug(f"JSON repair method {i+1} succeeded")
                    return repaired
            except (json.JSONDecodeError, Exception) as e:
                self.logger.debug(f"JSON repair method {i+1} failed: {e}")
                continue
        
        return None
    
    def _extract_core_json(self, text: str) -> str:
        """æå–æ–‡æœ¬ä¸­çš„æ ¸å¿ƒJSONéƒ¨åˆ†"""
        import re
        
        # æŸ¥æ‰¾æœ€å¤§çš„å®Œæ•´JSONå¯¹è±¡æˆ–æ•°ç»„
        patterns = [
            r'(\[[\s\S]*\])',  # æ•°ç»„
            r'(\{[\s\S]*\})',  # å¯¹è±¡
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            if matches:
                # è¿”å›æœ€é•¿çš„åŒ¹é…
                return max(matches, key=len)
        
        return text
    
    def _initialize_enhanced_clients(self):
        """åˆå§‹åŒ–å¢å¼ºå®¢æˆ·ç«¯ï¼ˆç”¨äºç»“æ„åŒ–è¾“å‡ºï¼‰"""
        for provider in self.providers:
            if provider.enabled:
                self.enhanced_clients[provider.name] = EnhancedLLMClient(provider.llm)
        
        self.logger.debug(f"Initialized {len(self.enhanced_clients)} enhanced clients")
    
    async def generate_structured_output(self,
                                       task_type: str,
                                       system_prompt: str,
                                       user_prompt: str,
                                       max_retries: int = 2) -> Optional[Any]:
        """
        ç”Ÿæˆç»“æ„åŒ–è¾“å‡º - ä½¿ç”¨å¢å¼ºçš„è§£æå™¨
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ (scene_splitting, image_prompt_generation, character_analysis, script_generation)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            ç»“æ„åŒ–è¾“å‡ºå¯¹è±¡ (Pydanticæ¨¡å‹)
        """
        
        # æ”¯æŒçš„ç»“æ„åŒ–ä»»åŠ¡ç±»å‹
        supported_tasks = ['scene_splitting', 'image_prompt_generation', 'character_analysis', 'script_generation']
        
        if task_type not in supported_tasks:
            self.logger.warning(f"Task type '{task_type}' not supported for structured output, falling back to regular generation")
            # é™çº§åˆ°æ™®é€šç”Ÿæˆ
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            return await self.generate_response(messages, task_type, expect_json=True)
        
        # å°è¯•å¢å¼ºå®¢æˆ·ç«¯
        last_error = None
        
        for provider_name, enhanced_client in self.enhanced_clients.items():
            try:
                self.logger.info(f"ğŸ”§ Trying structured generation with {provider_name}...")
                
                structured_output = await enhanced_client.generate_structured(
                    task_type=task_type,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    max_retries=max_retries
                )
                
                self.logger.info(f"âœ… Structured generation successful with {provider_name}")
                return structured_output
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"ğŸŒ Structured generation with {provider_name} failed: {e}")
                continue
        
        # æ‰€æœ‰å¢å¼ºå®¢æˆ·ç«¯éƒ½å¤±è´¥ï¼Œé™çº§åˆ°æ™®é€šç”Ÿæˆ
        self.logger.warning("ğŸ”„ All enhanced clients failed, falling back to regular JSON parsing")
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = await self.call_llm_with_fallback(messages, task_type)
            
            # å°è¯•æ‰‹åŠ¨è§£æä¸ºç»“æ„åŒ–æ ¼å¼
            return self._manual_structure_parsing(task_type, response)
            
        except Exception as fallback_error:
            self.logger.error(f"âŒ Even fallback generation failed: {fallback_error}")
            raise Exception(f"All generation methods failed. Last enhanced error: {last_error}, Fallback error: {fallback_error}")
    
    def _manual_structure_parsing(self, task_type: str, response_text: str) -> Optional[Any]:
        """æ‰‹åŠ¨ç»“æ„åŒ–è§£æ - ä½œä¸ºæœ€åçš„é™çº§æ–¹æ¡ˆ"""
        try:
            # å°è¯•è§£æJSON
            import json
            if isinstance(response_text, str):
                # æ¸…ç†å’Œæå–JSON
                cleaned_json = self._clean_and_extract_json(response_text)
                if cleaned_json:
                    parsed_data = json.loads(cleaned_json)
                    
                    # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºå¯¹åº”çš„ç»“æ„åŒ–å¯¹è±¡
                    if task_type == 'scene_splitting':
                        return SceneSplitOutput.model_validate(parsed_data)
                    elif task_type == 'image_prompt_generation':
                        return ImagePromptOutput.model_validate(parsed_data)
                    elif task_type == 'character_analysis':
                        return CharacterAnalysisOutput.model_validate(parsed_data)
                    elif task_type == 'script_generation':
                        return ScriptGenerationOutput.model_validate(parsed_data)
            
            return response_text  # è¿”å›åŸå§‹å“åº”
            
        except Exception as e:
            self.logger.warning(f"Manual structure parsing failed: {e}")
            return response_text
    
    def get_provider_stats(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_providers': len(self.providers),
            'enabled_providers': len([p for p in self.providers if p.enabled]),
            'provider_names': [p.name for p in self.providers if p.enabled],
            'gpt5_new_api_available': self.gpt5_client is not None
        }
    
    async def test_gpt5_new_api(self) -> Dict[str, Any]:
        """
        æµ‹è¯•GPT-5æ–°APIçš„å¯ç”¨æ€§
        
        Returns:
            DictåŒ…å«æµ‹è¯•ç»“æœ
        """
        if not self.gpt5_client:
            return {
                'available': False,
                'reason': 'GPT-5 client not initialized (missing API key)'
            }
        
        try:
            test_messages = [{"role": "user", "content": "Hello, this is a test message."}]
            response = await self.gpt5_client.create_response(
                messages=test_messages,
                model="openai/gpt-5",
                temperature=0.7,
                max_tokens=50
            )
            
            return {
                'available': True,
                'response_length': len(response) if response else 0,
                'api_format': 'new' if '/responses' in str(self.gpt5_client.base_url) else 'legacy'
            }
            
        except Exception as e:
            return {
                'available': False,
                'reason': str(e),
                'error_type': type(e).__name__
            }
    
    async def call_llm_with_fallback(self, 
                                   prompt: str, 
                                   task_type: str = 'script_generation',
                                   temperature: float = 0.8,
                                   max_tokens: int = 1024) -> Optional[str]:
        """
        å…¼å®¹æ€§æ–¹æ³• - ä½¿ç”¨fallbackæœºåˆ¶è°ƒç”¨LLM
        ä¿æŒä¸åŸLLMClientManageræ¥å£å…¼å®¹
        
        Args:
            prompt: æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            LLMå“åº”å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            config = ModelConfig(
                name="openai/gpt-5",  # é»˜è®¤ä½¿ç”¨GPT-5
                temperature=temperature,
                max_tokens=max_tokens,
                api_base="https://openrouter.ai/api/v1",
                api_key=""  # å°†ç”±LangChainç®¡ç†å™¨å¤„ç†
            )
            
            # è°ƒç”¨æ–°çš„å¼‚æ­¥æ–¹æ³•
            response = await self.call_llm_async(messages, config, task_type=task_type)
            return response
            
        except Exception as e:
            self.logger.error(f"call_llm_with_fallback failed: {e}")
            return None


# å…¼å®¹æ€§åˆ«å - ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§
class LLMClientManager(LangChainLLMManager):
    """å…¼å®¹æ€§åˆ«å - ä½¿ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨LangChain"""
    pass