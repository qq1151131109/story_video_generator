"""
åŸºäºLangChainçš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨ - æ”¯æŒGPT-5æ–°APIæ ¼å¼
ä¿æŒä¸åŸLLMClientManagerç›¸åŒçš„æ¥å£ï¼Œä½¿ç”¨LangChainåº•å±‚å®ç°
æ”¯æŒGPT-5çš„responses.create()å’Œä¼ ç»Ÿçš„chat.completions.create()ä¸¤ç§APIæ ¼å¼
"""
import asyncio
import logging
import os
import httpx
import json
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass

# LangChain imports
from langchain_core.language_models import BaseLLM, BaseLanguageModel
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser, BaseOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.exceptions import OutputParserException

# å¯¼å…¥åŸå§‹ModelConfigä»¥ä¿æŒå…¼å®¹æ€§
from core.config_manager import ModelConfig

@dataclass
class LangChainProvider:
    """LangChain LLMæä¾›å•†é…ç½®"""
    name: str
    llm: BaseLanguageModel
    models: Dict[str, str]
    enabled: bool = True

class GPT5NewAPIClient:
    """
    GPT-5æ–°APIæ ¼å¼å®¢æˆ·ç«¯
    ä½¿ç”¨responses.create()ç«¯ç‚¹è€Œä¸æ˜¯chat.completions.create()
    """
    
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.logger = logging.getLogger('story_generator.gpt5_api')
    
    async def create_response(self, 
                            messages: List[Dict[str, str]], 
                            model: str = "openai/gpt-5",
                            temperature: float = 0.8,
                            max_tokens: int = 1024) -> str:
        """
        ä½¿ç”¨GPT-5æ–°APIæ ¼å¼è°ƒç”¨LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            str: å“åº”å†…å®¹
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://story-video-generator.local",
            "X-Title": "Story Video Generator"
        }
        
        # GPT-5æ–°APIæ ¼å¼çš„è¯·æ±‚ä½“
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # å°è¯•æ–°çš„responsesç«¯ç‚¹
                response = await client.post(
                    f"{self.base_url}/responses",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    response_text = response.text
                    self.logger.debug(f"Raw response: {response_text[:200]}...")
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                    if not response_text or response_text.strip() == "":
                        self.logger.warning("Empty response from GPT-5 new API endpoint")
                        raise Exception("Empty response from GPT-5 new API endpoint")
                    
                    try:
                        result = response.json()
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Failed to parse JSON response: {e}, response: {response_text[:100]}")
                        # å°è¯•ä¼ ç»Ÿç«¯ç‚¹
                        return await self._fallback_to_legacy_api(messages, model, temperature, max_tokens, headers)
                    
                    # æ–°APIæ ¼å¼çš„å“åº”ç»“æ„
                    if "response" in result and "content" in result["response"]:
                        content = result["response"]["content"]
                        self.logger.info(f"âœ… GPT-5 new API call successful")
                        return content
                    elif "choices" in result and len(result["choices"]) > 0:
                        # å…¼å®¹ä¼ ç»Ÿå“åº”æ ¼å¼
                        content = result["choices"][0]["message"]["content"]
                        self.logger.info(f"âœ… GPT-5 new API call successful (legacy format)")
                        return content
                    else:
                        self.logger.warning(f"Unexpected response format: {result}")
                        return str(result)
                elif response.status_code == 404:
                    # æ–°ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œå°è¯•ä¼ ç»Ÿç«¯ç‚¹
                    self.logger.info("GPT-5 new API endpoint not found, falling back to legacy format")
                    return await self._fallback_to_legacy_api(messages, model, temperature, max_tokens, headers)
                else:
                    self.logger.error(f"GPT-5 API error {response.status_code}: {response.text}")
                    raise Exception(f"GPT-5 API error: {response.status_code}")
                    
        except httpx.TimeoutException:
            self.logger.error("GPT-5 API call timeout")
            raise Exception("GPT-5 API call timeout")
        except Exception as e:
            self.logger.error(f"GPT-5 API call failed: {e}")
            raise
    
    async def _fallback_to_legacy_api(self, 
                                    messages: List[Dict[str, str]], 
                                    model: str,
                                    temperature: float,
                                    max_tokens: int,
                                    headers: Dict[str, str]) -> str:
        """
        å›é€€åˆ°ä¼ ç»Ÿchat.completionsç«¯ç‚¹
        """
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    self.logger.info(f"âœ… GPT-5 legacy API call successful")
                    return content
                else:
                    raise Exception(f"Unexpected legacy response format: {result}")
            else:
                self.logger.error(f"GPT-5 legacy API error {response.status_code}: {response.text}")
                raise Exception(f"GPT-5 legacy API error: {response.status_code}")

class LangChainLLMManager:
    """
    åŸºäºLangChainçš„å¤šæä¾›å•†LLMç®¡ç†å™¨
    
    ä¼˜åŠ¿ï¼š
    1. ä½¿ç”¨LangChainçš„é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶
    2. å†…ç½®JSONè§£æå’ŒéªŒè¯
    3. æ›´å¥½çš„Promptç®¡ç†
    4. ä¿æŒä¸åŸæ¥å£100%å…¼å®¹
    """
    
    def __init__(self, config_manager):
        self.config = config_manager
        self.logger = logging.getLogger('story_generator.langchain_llm')
        
        # åˆå§‹åŒ–LangChainæä¾›å•†
        self.providers = self._initialize_langchain_providers()
        
        # ä¿ç•™GPT-5æ–°APIå®¢æˆ·ç«¯ä»¥å¤‡å°†æ¥ä½¿ç”¨
        self.gpt5_client = None
        if os.getenv('OPENROUTER_API_KEY'):
            self.gpt5_client = GPT5NewAPIClient(
                api_key=os.getenv('OPENROUTER_API_KEY'),
                base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')
            )
        
        # åˆå§‹åŒ–è¾“å‡ºè§£æå™¨
        self.json_parser = JsonOutputParser()
        
        gpt5_status = "enabled" if self.gpt5_client else "disabled"
        self.logger.info(f"LangChain LLM Manager initialized with {len(self.providers)} providers, GPT-5 API: {gpt5_status}")
    
    def _initialize_langchain_providers(self) -> List[LangChainProvider]:
        """åˆå§‹åŒ–LangChainæä¾›å•†ï¼Œä¼˜å…ˆçº§ï¼šOpenRouter(Gemini) > GPTsAPI(GPT-5) > DeepSeek"""
        providers = []
        
        # OpenRouteræä¾›å•†ï¼ˆé¦–é€‰ï¼Œä½¿ç”¨Geminiï¼‰
        if os.getenv('OPENROUTER_API_KEY'):
            openrouter_llm = ChatOpenAI(
                api_key=os.getenv('OPENROUTER_API_KEY'),
                base_url=os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1'),
                model="google/gemini-2.5-flash",  # ä¸»è¦æ¨¡å‹
                temperature=0.8,
                max_tokens=8192,
                timeout=30,
                max_retries=3
            )
            
            providers.append(LangChainProvider(
                name='openrouter',
                llm=openrouter_llm,
                models={task: 'google/gemini-2.5-flash' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
            ))
        
        # GPTsAPIæä¾›å•†ï¼ˆfallback GPT-5æä¾›å•†ï¼‰
        gptsapi_llm = ChatOpenAI(
            api_key='sk-yLda99082cd843c55e0453e3e23d384715143c3db7bmrz63',
            base_url='https://api.gptsapi.net/v1',
            model="gpt-5",  # GPTsAPIä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹åç§°
            temperature=0.8,
            max_tokens=8192,
            timeout=30,
            max_retries=3
        )
        
        providers.append(LangChainProvider(
            name='gptsapi',
            llm=gptsapi_llm,
            models={task: 'gpt-5' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
        ))
        
        # DeepSeekæä¾›å•†ï¼ˆå¤‡é€‰ï¼‰
        if os.getenv('DEEPSEEK_API_KEY'):
            deepseek_llm = ChatOpenAI(
                api_key=os.getenv('DEEPSEEK_API_KEY'),
                base_url=os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com'),
                model="deepseek-chat",
                temperature=0.8,
                max_tokens=8192,
                timeout=30,
                max_retries=3
            )
            
            providers.append(LangChainProvider(
                name='deepseek',
                llm=deepseek_llm,
                models={task: 'deepseek-chat' for task in ['script_generation', 'theme_extraction', 'scene_splitting', 'image_prompt_generation', 'character_analysis']}
            ))
        
        return providers
    
    async def call_llm_async(self, 
                           messages: List[Dict[str, str]], 
                           config: ModelConfig,
                           task_type: str = "general",
                           expect_json: bool = False) -> str:
        """
        å¼‚æ­¥è°ƒç”¨LLM - ä¿æŒä¸åŸæ¥å£å…¼å®¹ï¼Œæ”¯æŒGPT-5æ–°API
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            config: æ¨¡å‹é…ç½®
            task_type: ä»»åŠ¡ç±»å‹
            expect_json: æ˜¯å¦æœŸæœ›JSONå“åº”
            
        Returns:
            str: LLMå“åº”å†…å®¹
        """
        
        # GPT-5æ–°APIæš‚æ—¶ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨LangChainæä¾›å•†
        # OpenRouterçš„/responsesç«¯ç‚¹è¿˜æœªå®ç°ï¼ŒGPTsAPIå·²æä¾›GPT-5æ”¯æŒ
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºLangChainæ ¼å¼ï¼ˆç”¨äºä¼ ç»Ÿæä¾›å•†ï¼‰
        lc_messages = []
        for msg in messages:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            else:
                lc_messages.append(HumanMessage(content=msg["content"]))
        
        # å°è¯•LangChainæä¾›å•†
        last_error = None
        for provider in self.providers:
            if not provider.enabled:
                continue
                
            try:
                self.logger.debug(f"Trying provider {provider.name} with model {config.name}")
                
                # åŠ¨æ€æ›´æ–°æ¨¡å‹é…ç½®
                # æ ¹æ®æä¾›å•†é€‰æ‹©åˆé€‚çš„æ¨¡å‹
                if provider.name == 'deepseek':
                    model_to_use = 'deepseek-chat'
                elif provider.name == 'gptsapi':
                    # GPTsAPIä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹åç§°
                    if config.name == 'openai/gpt-5':
                        model_to_use = 'gpt-5'
                    else:
                        model_to_use = config.name
                elif provider.name == 'openrouter':
                    # OpenRouterä½œä¸ºfallbackï¼Œä½¿ç”¨Gemini
                    model_to_use = 'google/gemini-2.5-flash'
                else:
                    model_to_use = config.name
                
                provider.llm.model_name = model_to_use if hasattr(provider.llm, 'model_name') else model_to_use
                provider.llm.temperature = config.temperature
                provider.llm.max_tokens = config.max_tokens
                
                # è°ƒç”¨LLM
                if expect_json:
                    # å…ˆè·å–åŸå§‹å“åº”
                    result = await provider.llm.ainvoke(lc_messages)
                    response_text = result.content if hasattr(result, 'content') else str(result)
                    
                    # æ‰‹åŠ¨æ¸…ç†å’Œæå–JSON
                    try:
                        cleaned_json = self._clean_and_extract_json(response_text)
                        if cleaned_json:
                            # éªŒè¯JSONæ ¼å¼
                            parsed = json.loads(cleaned_json)
                            return json.dumps(parsed, ensure_ascii=False, indent=2)
                        else:
                            self.logger.warning(f"Could not extract valid JSON from response")
                            return response_text
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"JSON parsing failed: {e}")
                        return response_text
                else:
                    # æ™®é€šæ–‡æœ¬å“åº”
                    result = await provider.llm.ainvoke(lc_messages)
                    response_text = result.content if hasattr(result, 'content') else str(result)
                    
                    self.logger.debug(f"Raw response from {provider.name}: {repr(response_text)[:200]}...")
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                    if not response_text or response_text.strip() == "":
                        self.logger.warning(f"Empty response from provider: {provider.name}")
                        raise Exception(f"Empty response from provider: {provider.name}")
                    
                    self.logger.info(f"âœ… LLM call successful with provider: {provider.name}")
                    return response_text
                    
            except Exception as e:
                self.logger.warning(f"ğŸŒ Provider {provider.name} failed: {e}")
                last_error = e
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        error_msg = f"All LLM providers failed. Last error: {last_error}"
        self.logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    def call_llm(self, messages: List[Dict[str, str]], config: ModelConfig, **kwargs) -> str:
        """åŒæ­¥è°ƒç”¨LLM - ä¿æŒä¸åŸæ¥å£å…¼å®¹"""
        return asyncio.run(self.call_llm_async(messages, config, **kwargs))
    
    def create_chain_for_task(self, task_type: str, prompt_template: str = None) -> Any:
        """
        ä¸ºç‰¹å®šä»»åŠ¡åˆ›å»ºLangChain Chain
        è¿™æ˜¯LangChainç‰¹æœ‰çš„åŠŸèƒ½ï¼Œå¯é€‰ä½¿ç”¨
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            prompt_template: æç¤ºè¯æ¨¡æ¿
            
        Returns:
            LangChain Chainå¯¹è±¡
        """
        if not self.providers:
            raise RuntimeError("No providers available")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æä¾›å•†
        provider = self.providers[0]
        
        if prompt_template:
            # åˆ›å»ºå¸¦æç¤ºè¯æ¨¡æ¿çš„Chain
            prompt = ChatPromptTemplate.from_template(prompt_template)
            chain = prompt | provider.llm
        else:
            # ç®€å•Chain
            chain = provider.llm
        
        self.logger.info(f"Created chain for task: {task_type}")
        return chain
    
    def _clean_and_extract_json(self, response_text: str) -> Optional[str]:
        """æ¸…ç†å“åº”æ–‡æœ¬å¹¶æå–JSON"""
        import re
        import json
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', response_text)
        
        # æ–¹æ³•1: æŸ¥æ‰¾```json...```æ ¼å¼
        json_match = re.search(r'```json\s*\n?(.*?)\n?```', cleaned, re.DOTALL | re.IGNORECASE)
        if json_match:
            content = json_match.group(1).strip()
            try:
                json.loads(content)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                return content
            except json.JSONDecodeError:
                pass
        
        # æ–¹æ³•2: æŸ¥æ‰¾```...```æ ¼å¼  
        code_match = re.search(r'```\s*\n?(.*?)\n?```', cleaned, re.DOTALL)
        if code_match:
            content = code_match.group(1).strip()
            if content.startswith('{') and content.endswith('}'):
                try:
                    json.loads(content)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                    return content
                except json.JSONDecodeError:
                    pass
        
        # æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾JSONå¯¹è±¡
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, cleaned, re.DOTALL)
        
        for match in json_matches:
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and 'scenes' in parsed:
                    return match
            except json.JSONDecodeError:
                continue
        
        return None
    
    def get_provider_stats(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_providers': len(self.providers),
            'enabled_providers': len([p for p in self.providers if p.enabled]),
            'provider_names': [p.name for p in self.providers if p.enabled],
            'gpt5_new_api_available': self.gpt5_client is not None
        }
    
    async def test_gpt5_new_api(self) -> Dict[str, Any]:
        """
        æµ‹è¯•GPT-5æ–°APIçš„å¯ç”¨æ€§
        
        Returns:
            DictåŒ…å«æµ‹è¯•ç»“æœ
        """
        if not self.gpt5_client:
            return {
                'available': False,
                'reason': 'GPT-5 client not initialized (missing API key)'
            }
        
        try:
            test_messages = [{"role": "user", "content": "Hello, this is a test message."}]
            response = await self.gpt5_client.create_response(
                messages=test_messages,
                model="openai/gpt-5",
                temperature=0.7,
                max_tokens=50
            )
            
            return {
                'available': True,
                'response_length': len(response) if response else 0,
                'api_format': 'new' if '/responses' in str(self.gpt5_client.base_url) else 'legacy'
            }
            
        except Exception as e:
            return {
                'available': False,
                'reason': str(e),
                'error_type': type(e).__name__
            }
    
    async def call_llm_with_fallback(self, 
                                   prompt: str, 
                                   task_type: str = 'script_generation',
                                   temperature: float = 0.8,
                                   max_tokens: int = 1024) -> Optional[str]:
        """
        å…¼å®¹æ€§æ–¹æ³• - ä½¿ç”¨fallbackæœºåˆ¶è°ƒç”¨LLM
        ä¿æŒä¸åŸLLMClientManageræ¥å£å…¼å®¹
        
        Args:
            prompt: æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            LLMå“åº”å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            config = ModelConfig(
                name="openai/gpt-5",  # é»˜è®¤ä½¿ç”¨GPT-5
                temperature=temperature,
                max_tokens=max_tokens,
                api_base="https://openrouter.ai/api/v1",
                api_key=""  # å°†ç”±LangChainç®¡ç†å™¨å¤„ç†
            )
            
            # è°ƒç”¨æ–°çš„å¼‚æ­¥æ–¹æ³•
            response = await self.call_llm_async(messages, config, task_type=task_type)
            return response
            
        except Exception as e:
            self.logger.error(f"call_llm_with_fallback failed: {e}")
            return None


# å…¼å®¹æ€§åˆ«å - ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§
class LLMClientManager(LangChainLLMManager):
    """å…¼å®¹æ€§åˆ«å - ä½¿ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨LangChain"""
    pass