#!/usr/bin/env python3
"""
å¢å¼ºçš„LLMç®¡ç†å™¨ - æ”¯æŒOpenAI Structured Output + å¤šå±‚é™çº§
"""

import asyncio
import logging
import os
from typing import Type, TypeVar, Any, Dict, List, Optional, Union
from enum import Enum

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain.output_parsers import RetryOutputParser, OutputFixingParser
from pydantic import BaseModel

# å¯¼å…¥ç°æœ‰ç»„ä»¶
from core.config_manager import ConfigManager
from utils.structured_output_models import (
    SceneSplitOutput, ImagePromptOutput, CharacterAnalysisOutput, ScriptGenerationOutput
)

T = TypeVar('T', bound=BaseModel)

class ParseStrategy(Enum):
    """è§£æç­–ç•¥æšä¸¾"""
    STRUCTURED_OUTPUT = "structured_output"  # OpenAI Structured Output
    RETRY_PARSER = "retry_parser"            # é‡è¯•è§£æå™¨  
    OUTPUT_FIXING = "output_fixing"          # è¾“å‡ºä¿®å¤è§£æå™¨
    CUSTOM_ROBUST = "custom_robust"          # è‡ªå®šä¹‰é²æ£’è§£æ

class EnhancedLLMManager:
    """
    å¢å¼ºçš„LLMç®¡ç†å™¨
    
    ç‰¹ç‚¹:
    1. ä¼˜å…ˆä½¿ç”¨OpenRouterçš„OpenAI GPT-4.1 + Structured Output
    2. Geminiä½œä¸ºfallbackæ¨¡å‹
    3. RetryOutputParserä½œä¸ºé™çº§æ–¹æ¡ˆ
    4. å¤šå±‚è‡ªåŠ¨é™çº§æ¶æ„
    """
    
    def __init__(self, config: ConfigManager):
        self.config = config
        self.logger = logging.getLogger('enhanced_llm_manager')
        
        # è§£æå™¨ç¼“å­˜
        self._structured_models = {}
        self._retry_parsers = {}
        self._fixing_parsers = {}
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self._init_llm_clients()
        
        # åŠ è½½è§£æç­–ç•¥é…ç½®
        self.parsing_config = config.config.get('llm', {}).get('parsing_strategy', {})
        self.primary_strategy = self.parsing_config.get('primary', 'structured_output')
        self.fallback_strategies = self.parsing_config.get('fallback_strategies', 
                                                          ['retry_parser', 'output_fixing', 'custom_robust'])
        
        self.logger.info(f"âœ… å¢å¼ºLLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ¯ ä¸»è¦ç­–ç•¥: {self.primary_strategy}")
        self.logger.info(f"ğŸ”„ é™çº§ç­–ç•¥: {self.fallback_strategies}")
    
    def _init_llm_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        
        # è·å–OpenRouteré…ç½®
        openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
        openrouter_base_url = os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')
        
        if not openrouter_api_key:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°OPENROUTER_API_KEYç¯å¢ƒå˜é‡")
            self.primary_llm = None
            self.fallback_llm = None
            return
        
        # ä¸»LLM: OpenAI GPT-4.1 (é€šè¿‡OpenRouter)
        structured_config = self.config.config.get('llm', {}).get('structured_output', {})
        
        try:
            self.primary_llm = ChatOpenAI(
                model="openai/gpt-4.1",
                api_key=openrouter_api_key,
                base_url=openrouter_base_url,
                temperature=structured_config.get('temperature', 0.1),
                max_tokens=structured_config.get('max_tokens', 16384),
                timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´
                max_retries=3,  # å¢åŠ é‡è¯•æ¬¡æ•°
                request_timeout=60  # è¯·æ±‚è¶…æ—¶
            )
            self.logger.info("âœ… ä¸»LLM (OpenAI GPT-4.1) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ ä¸»LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            self.primary_llm = None
        
        # Fallback LLM: Gemini (é€šè¿‡OpenRouter)  
        try:
            self.fallback_llm = ChatOpenAI(
                model="google/gemini-2.5-flash",
                api_key=openrouter_api_key,
                base_url=openrouter_base_url,
                temperature=0.8,
                max_tokens=81920,
                timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´
                max_retries=3,  # å¢åŠ é‡è¯•æ¬¡æ•°
                request_timeout=60  # è¯·æ±‚è¶…æ—¶
            )
            self.logger.info("âœ… Fallback LLM (Gemini) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ Fallback LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            self.fallback_llm = None
    
    def get_structured_model(self, pydantic_model: Type[T]):
        """è·å–æ”¯æŒStructured Outputçš„æ¨¡å‹"""
        if not self.primary_llm:
            return None
            
        model_name = pydantic_model.__name__
        
        if model_name not in self._structured_models:
            try:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨strictæ¨¡å¼
                structured_config = self.config.config.get('llm', {}).get('structured_output', {})
                strict_mode = structured_config.get('strict_mode', True)
                
                # ä½¿ç”¨OpenAIå…¼å®¹çš„Structured Output
                # æ³¨æ„ï¼šOpenRouterå¯èƒ½ä¸æ”¯æŒstrict=Trueï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨function_callingæ–¹æ³•
                structured_model = self.primary_llm.with_structured_output(
                    pydantic_model,
                    method="function_calling"  # OpenRouteræ›´å…¼å®¹function calling
                )
                
                self._structured_models[model_name] = structured_model
                self.logger.info(f"âœ… åˆ›å»ºStructured Outputæ¨¡å‹: {model_name}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ— æ³•åˆ›å»ºStructured Outputæ¨¡å‹ {model_name}: {e}")
                return None
        
        return self._structured_models[model_name]
    
    def get_retry_parser(self, pydantic_model: Type[T]) -> Optional[RetryOutputParser]:
        """è·å–é‡è¯•è§£æå™¨"""
        if not self.fallback_llm:
            return None
            
        model_name = pydantic_model.__name__
        
        if model_name not in self._retry_parsers:
            try:
                base_parser = PydanticOutputParser(pydantic_object=pydantic_model)
                retry_parser = RetryOutputParser.from_llm(
                    parser=base_parser,
                    llm=self.fallback_llm,
                    max_retries=3
                )
                
                self._retry_parsers[model_name] = retry_parser
                self.logger.info(f"âœ… åˆ›å»ºRetryOutputParser: {model_name}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ— æ³•åˆ›å»ºRetryOutputParser {model_name}: {e}")
                return None
        
        return self._retry_parsers[model_name]
    
    def get_fixing_parser(self, pydantic_model: Type[T]) -> Optional[OutputFixingParser]:
        """è·å–ä¿®å¤è§£æå™¨"""
        if not self.fallback_llm:
            return None
            
        model_name = pydantic_model.__name__
        cache_key = f"{model_name}_fixing"
        
        if cache_key not in self._fixing_parsers:
            try:
                base_parser = PydanticOutputParser(pydantic_object=pydantic_model)
                fixing_parser = OutputFixingParser.from_llm(
                    parser=base_parser,
                    llm=self.fallback_llm
                )
                
                self._fixing_parsers[cache_key] = fixing_parser
                self.logger.info(f"âœ… åˆ›å»ºOutputFixingParser: {model_name}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ— æ³•åˆ›å»ºOutputFixingParser {model_name}: {e}")
                return None
        
        return self._fixing_parsers[cache_key]
    
    async def generate_structured_output(self,
                                       task_type: str,
                                       system_prompt: str,
                                       user_prompt: str,
                                       max_retries: int = 2) -> Any:
        """
        ç”Ÿæˆç»“æ„åŒ–è¾“å‡º - ä½¿ç”¨å¤šå±‚é™çº§ç­–ç•¥
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ ('scene_splitting', 'image_prompt_generation', etc.)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            è§£æåçš„ç»“æ„åŒ–å¯¹è±¡
        """
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©Pydanticæ¨¡å‹
        pydantic_model = self._get_pydantic_model_for_task(task_type)
        if not pydantic_model:
            raise ValueError(f"æœªæ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")
        
        # ä½¿ç”¨å¤šå±‚é™çº§ç­–ç•¥
        return await self._generate_with_auto_fallback(
            pydantic_model, system_prompt, user_prompt, max_retries
        )
    
    def _get_pydantic_model_for_task(self, task_type: str) -> Optional[Type[BaseModel]]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„Pydanticæ¨¡å‹"""
        model_mapping = {
            'scene_splitting': SceneSplitOutput,
            'image_prompt_generation': ImagePromptOutput,
            'character_analysis': CharacterAnalysisOutput,
            'script_generation': ScriptGenerationOutput,
        }
        return model_mapping.get(task_type)
    
    async def _generate_with_auto_fallback(self,
                                         pydantic_model: Type[T],
                                         system_prompt: str,
                                         user_prompt: str,
                                         max_retries: int = 2) -> T:
        """
        è‡ªåŠ¨é™çº§ç­–ç•¥ - ä»æœ€å¯é çš„æ–¹æ³•å¼€å§‹é€æ­¥é™çº§
        """
        
        errors = []
        
        # ç­–ç•¥1: OpenAI Structured Output (æœ€å¯é )
        if self.primary_strategy == "structured_output":
            try:
                self.logger.info("ğŸš€ å°è¯•OpenAI Structured Output...")
                result = await self._generate_with_structured_output(
                    pydantic_model, system_prompt, user_prompt
                )
                self.logger.info("âœ… OpenAI Structured Output æˆåŠŸ!")
                return result
            except Exception as e:
                error_msg = f"OpenAI Structured Outputå¤±è´¥: {e}"
                errors.append(error_msg)
                self.logger.warning(f"âš ï¸ {error_msg}")
        
        # é™çº§ç­–ç•¥æ‰§è¡Œ
        for strategy in self.fallback_strategies:
            try:
                if strategy == "retry_parser":
                    self.logger.info("ğŸ”„ é™çº§åˆ°RetryOutputParser...")
                    result = await self._generate_with_retry_parser(
                        pydantic_model, system_prompt, user_prompt
                    )
                    self.logger.info("âœ… RetryOutputParser æˆåŠŸ!")
                    return result
                    
                elif strategy == "output_fixing":
                    self.logger.info("ğŸ”§ é™çº§åˆ°OutputFixingParser...")
                    result = await self._generate_with_fixing_parser(
                        pydantic_model, system_prompt, user_prompt
                    )
                    self.logger.info("âœ… OutputFixingParser æˆåŠŸ!")
                    return result
                    
                elif strategy == "custom_robust":
                    self.logger.info("ğŸ“ é™çº§åˆ°è‡ªå®šä¹‰é²æ£’è§£æ...")
                    result = await self._generate_with_custom_robust(
                        pydantic_model, system_prompt, user_prompt
                    )
                    self.logger.info("âœ… è‡ªå®šä¹‰é²æ£’è§£æ æˆåŠŸ!")
                    return result
                    
            except Exception as e:
                error_msg = f"{strategy}å¤±è´¥: {e}"
                errors.append(error_msg)
                self.logger.warning(f"âš ï¸ {error_msg}")
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
        all_errors = "; ".join(errors)
        raise Exception(f"æ‰€æœ‰è§£æç­–ç•¥éƒ½å¤±è´¥: {all_errors}")
    
    async def _generate_with_structured_output(self, 
                                             pydantic_model: Type[T],
                                             system_prompt: str,
                                             user_prompt: str) -> T:
        """ä½¿ç”¨OpenAI Structured Outputç”Ÿæˆ"""
        structured_model = self.get_structured_model(pydantic_model)
        
        if not structured_model:
            raise Exception("OpenAI Structured Outputæ¨¡å‹æœªåˆå§‹åŒ–")
        
        # åˆ›å»ºæ¶ˆæ¯
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        # ä½¿ç”¨Structured Output
        result = await structured_model.ainvoke(messages)
        return result
    
    async def _generate_with_retry_parser(self,
                                        pydantic_model: Type[T],
                                        system_prompt: str,
                                        user_prompt: str) -> T:
        """ä½¿ç”¨RetryOutputParserç”Ÿæˆ"""
        retry_parser = self.get_retry_parser(pydantic_model)
        
        if not retry_parser:
            raise Exception("RetryOutputParseræœªåˆå§‹åŒ–")
        
        # å¢å¼ºæç¤ºè¯åŒ…å«æ ¼å¼è¯´æ˜
        base_parser = PydanticOutputParser(pydantic_object=pydantic_model)
        format_instructions = base_parser.get_format_instructions()
        
        enhanced_system_prompt = f"""
{system_prompt}

{format_instructions}

è¾“å‡ºè¦æ±‚: å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸¥æ ¼éµå¾ªä¸Šè¿°Schemaã€‚
"""
        
        # ç”Ÿæˆå®Œæ•´promptç”¨äºé‡è¯•ä¸Šä¸‹æ–‡
        full_prompt = f"System: {enhanced_system_prompt}\nUser: {user_prompt}"
        
        # è°ƒç”¨LLM
        messages = [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await self.fallback_llm.ainvoke(messages)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # ä½¿ç”¨RetryOutputParserè§£æ
        from langchain_core.prompt_values import StringPromptValue
        prompt_value = StringPromptValue(text=full_prompt)
        
        result = retry_parser.parse_with_prompt(response_text, prompt_value)
        return result
    
    async def _generate_with_fixing_parser(self,
                                         pydantic_model: Type[T],
                                         system_prompt: str,
                                         user_prompt: str) -> T:
        """ä½¿ç”¨OutputFixingParserç”Ÿæˆ"""
        fixing_parser = self.get_fixing_parser(pydantic_model)
        
        if not fixing_parser:
            raise Exception("OutputFixingParseræœªåˆå§‹åŒ–")
        
        # å¢å¼ºæç¤ºè¯
        base_parser = PydanticOutputParser(pydantic_object=pydantic_model)
        format_instructions = base_parser.get_format_instructions()
        
        enhanced_system_prompt = f"""
{system_prompt}

{format_instructions}

è¾“å‡ºè¦æ±‚: å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸¥æ ¼éµå¾ªä¸Šè¿°Schemaã€‚
"""
        
        # è°ƒç”¨LLM
        messages = [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await self.fallback_llm.ainvoke(messages)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # ä½¿ç”¨OutputFixingParserè§£æ
        result = fixing_parser.parse(response_text)
        return result
    
    async def _generate_with_custom_robust(self,
                                         pydantic_model: Type[T],
                                         system_prompt: str,
                                         user_prompt: str) -> T:
        """ä½¿ç”¨è‡ªå®šä¹‰é²æ£’è§£æå™¨ç”Ÿæˆ"""
        from utils.robust_output_parser import RobustStructuredOutputParser
        
        parser = RobustStructuredOutputParser(pydantic_model)
        
        # ä½¿ç”¨ä»»ä½•å¯ç”¨çš„LLM
        llm = self.fallback_llm or self.primary_llm
        if not llm:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„LLM")
        
        # å¢å¼ºæç¤ºè¯
        base_parser = PydanticOutputParser(pydantic_object=pydantic_model)
        format_instructions = base_parser.get_format_instructions()
        
        enhanced_system_prompt = f"""
{system_prompt}

{format_instructions}

è¾“å‡ºè¦æ±‚: å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸¥æ ¼éµå¾ªä¸Šè¿°Schemaã€‚
"""
        
        messages = [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await llm.ainvoke(messages)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # ä½¿ç”¨è‡ªå®šä¹‰é²æ£’è§£æå™¨
        result = parser.parse(response_text)
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            "primary_model": "openai/gpt-4.1" if self.primary_llm else None,
            "fallback_model": "google/gemini-2.5-flash" if self.fallback_llm else None,
            "primary_strategy": self.primary_strategy,
            "fallback_strategies": self.fallback_strategies,
            "structured_output_enabled": bool(self.primary_llm),
            "retry_parser_enabled": bool(self.fallback_llm)
        }

    async def call_llm_with_fallback(self,
                                   prompt: str,
                                   task_type: str,
                                   temperature: Optional[float] = None,
                                   max_tokens: Optional[int] = None,
                                   **kwargs) -> str:
        """
        å‘åå…¼å®¹æ–¹æ³• - é€‚é…æ—§çš„call_llm_with_fallbackæ¥å£
        
        è¿™ä¸ªæ–¹æ³•å°†promptåˆ†è§£ä¸ºç³»ç»Ÿå’Œç”¨æˆ·æç¤ºè¯ï¼Œç„¶åè°ƒç”¨generate_structured_output
        """
        # ç®€å•çš„æç¤ºè¯åˆ†å‰²ç­–ç•¥
        # å¦‚æœpromptåŒ…å«æ˜ç¡®çš„system/useråˆ†éš”ï¼Œå°è¯•è¯†åˆ«
        if "System:" in prompt and "User:" in prompt:
            parts = prompt.split("User:", 1)
            system_prompt = parts[0].replace("System:", "").strip()
            user_prompt = parts[1].strip()
        else:
            # å¦åˆ™å°†æ•´ä¸ªpromptä½œä¸ºç”¨æˆ·æç¤ºè¯ï¼Œä½¿ç”¨ç©ºçš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = ""
            user_prompt = prompt
        
        try:
            # è°ƒç”¨ç»“æ„åŒ–è¾“å‡ºæ–¹æ³•
            result = await self.generate_structured_output(
                task_type=task_type,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_retries=2
            )
            
            # å¦‚æœè¿”å›çš„æ˜¯ç»“æ„åŒ–å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºJSONæ ¼å¼
            if hasattr(result, 'model_dump'):
                # Pydanticæ¨¡å‹ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                import json
                return json.dumps(result.model_dump(), ensure_ascii=False, indent=2)
            elif hasattr(result, 'content'):
                return result.content
            elif hasattr(result, 'text'):
                return result.text
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬ä¸ºå­—ç¬¦ä¸²
                return str(result)
                
        except Exception as e:
            self.logger.error(f"call_llm_with_fallback compatibility method failed: {e}")
            raise


# æµ‹è¯•å‡½æ•°
async def test_enhanced_llm_manager():
    """æµ‹è¯•å¢å¼ºçš„LLMç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºçš„LLMç®¡ç†å™¨")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–
        config = ConfigManager()
        llm_manager = EnhancedLLMManager(config)
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        info = llm_manager.get_model_info()
        print("ğŸ“Š æ¨¡å‹é…ç½®ä¿¡æ¯:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # æµ‹è¯•åœºæ™¯åˆ†å‰²
        print("\nğŸ¬ æµ‹è¯•åœºæ™¯åˆ†å‰²...")
        result = await llm_manager.generate_structured_output(
            task_type='scene_splitting',
            system_prompt="ä½ æ˜¯ä¸“ä¸šçš„æ•…äº‹åœºæ™¯åˆ†å‰²ä¸“å®¶ã€‚å°†è¾“å…¥çš„æ•…äº‹åˆ†å‰²ä¸ºå¤šä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯3ç§’é’Ÿã€‚",
            user_prompt="è¯·å°†ç§¦å§‹çš‡ç»Ÿä¸€å¤©ä¸‹çš„æ•…äº‹åˆ†å‰²ä¸º5ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«ä¸åŒçš„é‡è¦æƒ…èŠ‚ç‚¹ã€‚"
        )
        
        if hasattr(result, 'scenes'):
            print(f"âœ… æˆåŠŸè§£æ {len(result.scenes)} ä¸ªåœºæ™¯:")
            for scene in result.scenes[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"   åœºæ™¯{scene.sequence}: {scene.content[:50]}...")
        else:
            print(f"âš ï¸ ç»“æœæ ¼å¼å¼‚å¸¸: {type(result)}")
            
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import asyncio
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_enhanced_llm_manager())