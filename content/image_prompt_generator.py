"""
å›¾åƒæç¤ºè¯ç”Ÿæˆå™¨ - åŸºäºåŸCozeå·¥ä½œæµNode_186126çš„å®Œæ•´é€»è¾‘
å¯¹åº”åŸå·¥ä½œæµNode_186126é…ç½®
"""
import asyncio
import json
import time
from typing import Dict, List, Optional, Any
from pathlib import Path
import logging
from dataclasses import dataclass

from core.config_manager import ConfigManager, ModelConfig
from utils.file_manager import FileManager
from utils.enhanced_llm_manager import EnhancedLLMManager
from .scene_splitter import Scene

@dataclass
class ImagePromptRequest:
    """å›¾åƒæç¤ºè¯ç”Ÿæˆè¯·æ±‚"""
    scenes: List[Scene]      # åœºæ™¯åˆ—è¡¨ï¼ˆåŒ…å«capå­—å¹•æ–‡æ¡ˆï¼‰
    language: str           # è¯­è¨€ä»£ç 
    style: str = "ancient_horror"  # è§†è§‰é£æ ¼

@dataclass
class ImagePromptResult:
    """å›¾åƒæç¤ºè¯ç”Ÿæˆç»“æœ"""
    scenes: List[Scene]      # æ›´æ–°åçš„åœºæ™¯åˆ—è¡¨ï¼ˆåŒ…å«image_promptï¼‰
    language: str           # è¯­è¨€
    generation_time: float  # ç”Ÿæˆè€—æ—¶
    model_used: str         # ä½¿ç”¨çš„æ¨¡å‹

class ImagePromptGenerator:
    """
    å›¾åƒæç¤ºè¯ç”Ÿæˆå™¨
    
    åŸºäºåŸCozeå·¥ä½œæµNode_186126é…ç½®ï¼š
    - æ¨¡å‹: DeepSeek-V3-0324
    - Temperature: 1.0
    - Max tokens: 16384
    - ä¸ºæ¯ä¸ªåˆ†é•œç”Ÿæˆè¯¦ç»†çš„å›¾åƒç»˜ç”»æç¤ºè¯
    """
    
    def __init__(self, config_manager: ConfigManager, 
                 file_manager: FileManager):
        self.config = config_manager
        self.file_manager = file_manager
        self.logger = logging.getLogger('story_generator.content')
        
        # æ”¯æŒçš„è¯­è¨€
        self.supported_languages = self.config.get_supported_languages()
        
        # è·å–LLMé…ç½® - ä½¿ç”¨ä¸“é—¨çš„å›¾åƒæç¤ºè¯ç”Ÿæˆé…ç½®
        self.llm_config = self.config.get_llm_config('image_prompt_generation')
        
        # åˆå§‹åŒ–å¤šæä¾›å•†LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
        self.llm_manager = EnhancedLLMManager(config_manager)
        self.logger.info("âœ… ä½¿ç”¨å¢å¼ºLLMç®¡ç†å™¨ (ç»Ÿä¸€æ¶æ„)")
        
        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        self._load_prompt_templates()
        
        self.logger.info("Image prompt generator initialized")
    
    
    async def generate_image_prompts_async(self, request: ImagePromptRequest) -> ImagePromptResult:
        """
        å¼‚æ­¥ç”Ÿæˆå›¾åƒæç¤ºè¯
        
        Args:
            request: å›¾åƒæç¤ºè¯ç”Ÿæˆè¯·æ±‚
        
        Returns:
            ImagePromptResult: ç”Ÿæˆç»“æœ
        """
        start_time = time.time()
        
        try:
            # ç¼“å­˜å·²ç¦ç”¨ - æ¯æ¬¡éƒ½ç”Ÿæˆæ–°å†…å®¹
            
            # éªŒè¯è¯·æ±‚
            if request.language not in self.supported_languages:
                raise ValueError(f"Unsupported language: {request.language}")
            
            # æ„å»ºæç¤ºè¯ - åŸºäºåŸå·¥ä½œæµNode_186126
            prompt = self._build_image_prompt_generation_prompt(request)
            
            # è°ƒç”¨LLM API
            self.logger.info(f"Generating image prompts for {len(request.scenes)} scenes...")
            
            response = await self._call_llm_api(prompt)
            
            if not response:
                raise ValueError("Empty response from LLM")
            
            # è§£æå“åº”
            updated_scenes = self._parse_image_prompt_response(response, request.scenes)
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = ImagePromptResult(
                scenes=updated_scenes,
                language=request.language,
                generation_time=time.time() - start_time,
                model_used=self.llm_config.name
            )
            
            # ç¼“å­˜å·²ç¦ç”¨
            
            # è®°å½•æ—¥å¿—
            logger = self.config.get_logger('story_generator')
            logger.info(f"Content generation - Type: image_prompt_generation, Language: {request.language}, "
                       f"Scenes: {len(request.scenes)}, Time: {result.generation_time:.2f}s")
            
            self.logger.info(f"Generated image prompts successfully for {len(updated_scenes)} scenes in {result.generation_time:.2f}s")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"Image prompt generation failed: {e}")
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            logger = self.config.get_logger('story_generator')
            logger.error(f"Content generation failed - Type: image_prompt_generation, Language: {request.language}, "
                        f"Scenes: {len(request.scenes)}, Time: {processing_time:.2f}s")
            
            raise
    
    def _build_image_prompt_generation_prompt(self, request: ImagePromptRequest) -> str:
        """
        æ„å»ºå›¾åƒæç¤ºè¯ç”Ÿæˆæç¤ºè¯ - ä½¿ç”¨æ¨¡æ¿æ–‡ä»¶
        """
        # å‡†å¤‡åœºæ™¯æ•°æ®
        scenes_json = []
        for scene in request.scenes:
            scenes_json.append({
                "cap": scene.content,
                "image_prompt": "",  # å¾…å¡«å……ï¼Œä½¿ç”¨æ–°çš„æ ‡å‡†å­—æ®µå
                "video_prompt": ""   # å¾…å¡«å……
            })
        
        scenes_json_str = json.dumps(scenes_json, ensure_ascii=False, indent=2)
        
        # ä½¿ç”¨æ¨¡æ¿æ–‡ä»¶
        language = request.language
        if language in self.prompt_templates:
            system_prompt = self.prompt_templates[language]
        else:
            # ä½¿ç”¨ä¸­æ–‡ä½œä¸ºåå¤‡
            system_prompt = self.prompt_templates.get('zh', '')
            self.logger.warning(f"No image prompt template for language {language}, using Chinese as fallback")

        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        prompt = system_prompt.replace("{{scenes}}", scenes_json_str)
        
        return prompt
    
    def _split_prompt(self, full_prompt: str) -> tuple[str, str]:
        """
        åˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
        
        Args:
            full_prompt: å®Œæ•´æç¤ºè¯
            
        Returns:
            tuple: (system_prompt, user_prompt)
        """
        # æŸ¥æ‰¾JSONåœºæ™¯æ•°æ®çš„å¼€å§‹ä½ç½®
        json_start = full_prompt.find('[')
        if json_start == -1:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°†æ•´ä¸ªå†…å®¹ä½œä¸ºç”¨æˆ·æç¤ºè¯
            return "ä½ æ˜¯ä¸“ä¸šçš„å›¾åƒæç¤ºè¯ç”Ÿæˆä¸“å®¶ã€‚", full_prompt
        
        system_part = full_prompt[:json_start].strip()
        user_part = full_prompt[json_start:].strip()
        
        if not system_part:
            system_part = "ä½ æ˜¯ä¸“ä¸šçš„å›¾åƒæç¤ºè¯ç”Ÿæˆä¸“å®¶ã€‚"
            
        return system_part, user_part

    async def _call_llm_api(self, prompt: str) -> str:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: æç¤ºè¯
        
        Returns:
            str: LLMå“åº”
        """
        try:
            # å°è¯•ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            try:
                # åˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
                system_prompt, user_prompt = self._split_prompt(prompt)
                
                structured_output = await self.llm_manager.generate_structured_output(
                    task_type='image_prompt_generation',
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    max_retries=2
                )
                
                if hasattr(structured_output, 'scenes') and structured_output.scenes:
                    # ç›´æ¥è¿”å›ç»“æ„åŒ–å†…å®¹ï¼Œè®©åç»­å¤„ç†é€»è¾‘å¤„ç†
                    content = self._convert_structured_to_json(structured_output.scenes)
                    self.logger.info(f"âœ… Structured image prompt generation successful: {len(structured_output.scenes)} scenes")
                else:
                    # é™çº§åˆ°åŸæœ‰æ–¹æ³•
                    content = str(structured_output)
                    
            except Exception as e:
                self.logger.warning(f"ğŸ”„ Structured output failed, falling back to regular parsing: {e}")
                
                # é™çº§åˆ°åŸæœ‰æ–¹æ³•
                content = await self.llm_manager.call_llm_with_fallback(
                    prompt=prompt,
                    task_type='image_prompt_generation',
                    temperature=self.llm_config.temperature,
                    max_tokens=self.llm_config.max_tokens
                )
            
            if not content:
                raise ValueError("Empty response from all LLM providers")
            
            return content.strip()
            
        except Exception as e:
            self.logger.error(f"LLM API call failed: {e}")
            raise
    
    def _convert_structured_to_json(self, scenes_data) -> str:
        """å°†ç»“æ„åŒ–è¾“å‡ºè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        import json
        
        scenes_list = []
        for scene_data in scenes_data:
            scene_dict = {
                "cap": getattr(scene_data, 'content', ''),  # å…¼å®¹åŸæœ‰å­—æ®µå
                "image_prompt": getattr(scene_data, 'image_prompt', ''),  # ä½¿ç”¨æ–°çš„æ ‡å‡†å­—æ®µå
                "video_prompt": getattr(scene_data, 'video_prompt', '')
            }
            scenes_list.append(scene_dict)
        
        return json.dumps(scenes_list, ensure_ascii=False, indent=2)
    
    def _parse_image_prompt_response(self, response: str, original_scenes: List[Scene]) -> List[Scene]:
        """
        è§£æå›¾åƒæç¤ºè¯å“åº”
        
        Args:
            response: LLMå“åº”
            original_scenes: åŸå§‹åœºæ™¯åˆ—è¡¨
        
        Returns:
            List[Scene]: æ›´æ–°åçš„åœºæ™¯åˆ—è¡¨
        """
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_content = self._extract_json_from_response(response)
            
            if not json_content:
                raise ValueError("No valid JSON found in response")
            
            data = json.loads(json_content)
            
            if not isinstance(data, list):
                raise ValueError("Response should be a list of scene objects")
            
            updated_scenes = []
            
            for i, (original_scene, prompt_data) in enumerate(zip(original_scenes, data)):
                if not isinstance(prompt_data, dict):
                    raise ValueError(f"Scene {i+1} data should be an object")
                
                # è·å–ç”Ÿæˆçš„è‹±æ–‡æç¤ºè¯ - æ”¯æŒå¤šç§å­—æ®µå(æ–°æ—§å…¼å®¹)
                image_prompt = prompt_data.get('image_prompt', 
                                             prompt_data.get('desc_prompt', 
                                                           prompt_data.get('desc_promopt', ''))).strip()
                
                # è·å–è§†é¢‘æç¤ºè¯
                video_prompt = prompt_data.get('video_prompt', '').strip()
                
                # éªŒè¯å›¾åƒæç¤ºè¯è´¨é‡
                if not image_prompt:
                    raise ValueError(f"Empty image prompt for scene {i+1}")
                
                if len(image_prompt) < 30:
                    raise ValueError(f"Image prompt too short for scene {i+1}: {image_prompt}")
                
                # éªŒè¯è§†é¢‘æç¤ºè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if video_prompt and len(video_prompt) < 10:
                    self.logger.warning(f"Scene {i+1} video prompt might be too short: {video_prompt}")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼ˆåº”è¯¥æ˜¯è‹±æ–‡ï¼‰
                if any(ord(char) > 127 for char in image_prompt):
                    self.logger.warning(f"Scene {i+1} image prompt contains non-ASCII characters: {image_prompt[:50]}...")
                
                # åˆ›å»ºæ›´æ–°åçš„åœºæ™¯
                updated_scene = Scene(
                    sequence=original_scene.sequence,
                    content=original_scene.content,
                    image_prompt=image_prompt,
                    video_prompt=video_prompt if video_prompt else getattr(original_scene, 'video_prompt', ''),  # ä½¿ç”¨æ–°ç”Ÿæˆçš„video_prompt
                    duration_seconds=original_scene.duration_seconds,
                    animation_type=original_scene.animation_type,
                    subtitle_text=original_scene.subtitle_text
                )
                
                updated_scenes.append(updated_scene)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åœºæ™¯éƒ½å¤„ç†äº†
            if len(updated_scenes) != len(original_scenes):
                self.logger.warning(f"Mismatch: {len(original_scenes)} input scenes, {len(updated_scenes)} output scenes")
            
            # æ£€æŸ¥é‡å¤æç¤ºè¯
            prompts = [scene.image_prompt for scene in updated_scenes]
            unique_prompts = set(prompts)
            if len(unique_prompts) < len(prompts):
                self.logger.warning(f"Found {len(prompts) - len(unique_prompts)} duplicate image prompts")
            
            return updated_scenes
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON parsing error: {e}")
            self.logger.error(f"Raw LLM response that caused JSON error: {response[:1000]}...")
            raise ValueError(f"LLM returned invalid JSON format: {e}")
        except Exception as e:
            self.logger.error(f"Image prompt parsing error: {e}")
            self.logger.error(f"Raw LLM response: {response[:1000]}...")
            raise
    
    def _extract_json_from_response(self, response: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
        import re
        
        # æŸ¥æ‰¾```json...```æ ¼å¼
        json_match = re.search(r'```json\s*\n?(.*?)\n?```', response, re.DOTALL | re.IGNORECASE)
        if json_match:
            return json_match.group(1).strip()
        
        # æŸ¥æ‰¾```...```æ ¼å¼ï¼ˆå¯èƒ½æ²¡æœ‰æ ‡æ˜jsonï¼‰
        code_match = re.search(r'```\s*\n?(.*?)\n?```', response, re.DOTALL)
        if code_match:
            content = code_match.group(1).strip()
            if content.startswith('[') and content.endswith(']'):
                return content
        
        # æŸ¥æ‰¾ç›´æ¥çš„JSONæ•°ç»„
        json_array_match = re.search(r'(\[.*?\])', response, re.DOTALL)
        if json_array_match:
            return json_array_match.group(1)
        
        # æœ€åå°è¯•ç®€å•çš„æ–¹æ‹¬å·åŒ¹é…
        start_pos = response.find('[')
        if start_pos != -1:
            bracket_count = 0
            for i, char in enumerate(response[start_pos:], start_pos):
                if char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        return response[start_pos:i+1]
        
        return None
    
    # FALLBACK LOGIC REMOVED - ä¸å†ä½¿ç”¨é€€åŒ–é€»è¾‘æ©ç›–é—®é¢˜
    
    def _scene_to_dict(self, scene: Scene) -> Dict[str, Any]:
        """å°†Sceneå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'sequence': scene.sequence,
            'content': scene.content,
            'image_prompt': scene.image_prompt,
            'video_prompt': getattr(scene, 'video_prompt', ''),
            'duration_seconds': scene.duration_seconds,
            'animation_type': scene.animation_type,
            'subtitle_text': scene.subtitle_text
        }
    
    def generate_image_prompts_sync(self, request: ImagePromptRequest) -> ImagePromptResult:
        """
        åŒæ­¥ç”Ÿæˆå›¾åƒæç¤ºè¯ï¼ˆå¯¹å¼‚æ­¥æ–¹æ³•çš„åŒ…è£…ï¼‰
        
        Args:
            request: å›¾åƒæç¤ºè¯ç”Ÿæˆè¯·æ±‚
        
        Returns:
            ImagePromptResult: ç”Ÿæˆç»“æœ
        """
        return asyncio.run(self.generate_image_prompts_async(request))
    
    def get_generation_stats(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        return {
            'supported_languages': self.supported_languages,
            # ç¼“å­˜å·²åˆ é™¤
            'model_config': {
                'name': self.llm_config.name,
                'temperature': self.llm_config.temperature,
                'max_tokens': self.llm_config.max_tokens
            }
        }
    
    def _load_prompt_templates(self):
        """åŠ è½½æç¤ºè¯æ¨¡æ¿"""
        self.prompt_templates = {}
        prompts_dir = Path("config/prompts")
        
        for lang in self.supported_languages:
            lang_dir = prompts_dir / lang
            template_file = lang_dir / "image_prompt_generation.txt"
            
            if template_file.exists():
                try:
                    with open(template_file, 'r', encoding='utf-8') as f:
                        self.prompt_templates[lang] = f.read().strip()
                    self.logger.debug(f"Loaded image prompt template for language: {lang}")
                except Exception as e:
                    self.logger.warning(f"Failed to load image prompt template for {lang}: {e}")
            else:
                self.logger.warning(f"Image prompt template not found: {template_file}")
        
        self.logger.info(f"Loaded {len(self.prompt_templates)} image prompt templates")

    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"ImagePromptGenerator(model={self.llm_config.name}, languages={self.supported_languages})"