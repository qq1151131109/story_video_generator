"""
ä¸€ä½“åŒ–æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆå™¨ - åŸºäºRunningHubæ–°å·¥ä½œæµ
å·¥ä½œæµID: 1964196221642489858

å°†æ–‡ç”Ÿå›¾+å›¾ç”Ÿè§†é¢‘æ•´åˆä¸ºå•ä¸€APIè°ƒç”¨ï¼Œæ›¿ä»£ç°æœ‰çš„ä¸¤æ­¥æµç¨‹ï¼š
- åŸæµç¨‹: ImageGenerator â†’ ImageToVideoGenerator (2æ¬¡APIè°ƒç”¨)
- æ–°æµç¨‹: TextToVideoGenerator (1æ¬¡APIè°ƒç”¨)

å¯¹åº”å·¥ä½œæµèŠ‚ç‚¹é…ç½®ï¼š
- æ–‡æœ¬ç¼–ç : nodes 1,10,38 (CLIPTextEncode)
- å›¾åƒç”Ÿæˆ: Fluxæ¨¡å‹ via UNET nodes 13,14,37
- å›¾ç”Ÿè§†é¢‘: Wan2.2æ¨¡å‹ via WanImageToVideo node 5
- è§†é¢‘è¾“å‡º: 720x1280@31fps, MP4æ ¼å¼
"""

import asyncio
import json
import time
import uuid
from pathlib import Path
from typing import Dict, Optional, Any, List
import logging
from dataclasses import dataclass

import aiohttp
from PIL import Image

from core.config_manager import ConfigManager
from utils.file_manager import FileManager

# ğŸŒ æ¨¡å—çº§åˆ«çš„å…¨å±€ä¿¡å·é‡æ§åˆ¶ - è§£å†³å¤šå®ä¾‹å¹¶å‘é—®é¢˜
_global_semaphore = None
_global_semaphore_lock = asyncio.Lock()

async def get_global_semaphore(max_concurrent: int) -> asyncio.Semaphore:
    """è·å–æˆ–åˆ›å»ºå…¨å±€ä¿¡å·é‡"""
    global _global_semaphore
    
    async with _global_semaphore_lock:
        if _global_semaphore is None or _global_semaphore._value != max_concurrent:
            _global_semaphore = asyncio.Semaphore(max_concurrent)
            logging.getLogger('story_generator.text_to_video').info(
                f"ğŸŒ Created module-level global semaphore with {max_concurrent} concurrent limit"
            )
    
    return _global_semaphore


@dataclass
class TextToVideoRequest:
    """æ–‡ç”Ÿè§†é¢‘è¯·æ±‚"""
    image_prompt: str              # æ–‡ç”Ÿå›¾æç¤ºè¯ (node 38)
    video_prompt: str = ""         # å›¾ç”Ÿè§†é¢‘æç¤ºè¯ (node 10)  
    negative_prompt: str = ""      # è´Ÿå‘æç¤ºè¯ (node 1)
    width: int = 720              # è§†é¢‘å®½åº¦
    height: int = 1280            # è§†é¢‘é«˜åº¦
    fps: int = 31                 # å¸§ç‡
    duration: float = 3.0         # æŒç»­æ—¶é—´(ç§’)
    style: str = "ancient_horror" # é£æ ¼æ ‡è¯†
    scene_id: str = ""            # åœºæ™¯ID
    seed: int = -1                # éšæœºç§å­(-1ä¸ºéšæœº)


@dataclass
class TextToVideoResult:
    """æ–‡ç”Ÿè§†é¢‘ç»“æœ"""
    video_path: str               # è§†é¢‘æ–‡ä»¶è·¯å¾„
    width: int                    # è§†é¢‘å®½åº¦
    height: int                   # è§†é¢‘é«˜åº¦
    fps: int                      # å¸§ç‡
    duration: float               # å®é™…æ—¶é•¿
    file_size: int                # æ–‡ä»¶å¤§å°
    provider: str = "runninghub"  # æä¾›å•†
    workflow_id: str = ""         # å·¥ä½œæµID
    task_id: str = ""             # ä»»åŠ¡ID
    generation_time: float = 0.0  # ç”Ÿæˆè€—æ—¶


class TextToVideoGenerator:
    """
    ä¸€ä½“åŒ–æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆå™¨
    
    ä½¿ç”¨RunningHubæ–°å·¥ä½œæµ(1964196221642489858)å®ç°ï¼š
    æ–‡æœ¬æç¤ºè¯ â†’ å›¾åƒç”Ÿæˆ(Flux) â†’ è§†é¢‘è½¬æ¢(Wan2.2) â†’ MP4è¾“å‡º
    
    æ›¿ä»£åŸæœ‰çš„ä¸¤æ­¥æµç¨‹ï¼Œæé«˜æ•ˆç‡å¹¶å‡å°‘APIè°ƒç”¨æ¬¡æ•°ã€‚
    """
    
    def __init__(self, config: ConfigManager, file_manager: FileManager):
        self.config = config
        self.file_manager = file_manager
        self.logger = logging.getLogger('story_generator.text_to_video')
        
        # å·¥ä½œæµé…ç½® - ä¸€ä½“åŒ–æ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ
        self.workflow_id = "1964196221642489858"
        
        # ğŸŒ æ¨¡å—çº§å…¨å±€å¹¶å‘æ§åˆ¶ - è§£å†³å¤šå®ä¾‹é—®é¢˜
        self._max_concurrent = self.config.get('general.max_concurrent_tasks', 3)
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œåˆ›å»ºä¿¡å·é‡ï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶è·å–å…¨å±€ä¿¡å·é‡
        self.logger.info(f"ğŸŒ TextToVideoGenerator will use module-level global semaphore with {self._max_concurrent} concurrent limit")
        
        # APIé…ç½® - ä½¿ç”¨ä¸å›¾åƒç”Ÿæˆå™¨ç›¸åŒçš„APIç«¯ç‚¹
        self.api_key = self.config.get_api_key('runninghub')
        self.base_url = "https://www.runninghub.cn"
        
        # è¶…æ—¶å’Œé‡è¯•é…ç½®
        self.connect_timeout = self.config.get('general.connect_timeout', 30)  # è¿æ¥è¶…æ—¶
        self.request_timeout = self.config.get('general.request_timeout', 60)  # è¯·æ±‚è¶…æ—¶
        self.total_timeout = self.config.get('general.api_timeout', 600)  # æ€»è¶…æ—¶ï¼Œæ”¹ä¸º10åˆ†é’Ÿ
        self.max_retries = self.config.get('general.api_max_retries', 5)  # å¢åŠ é‡è¯•æ¬¡æ•°
        self.retry_delay = self.config.get('general.retry_delay', 10)  # å¢åŠ é‡è¯•é—´éš”
        
        if not self.api_key:
            raise ValueError("RunningHub API key not configured")
        
        # ğŸŒ ä¼˜åŒ–è¿æ¥é…ç½®æ”¯æŒ5ä¸ªå¹¶å‘ä»»åŠ¡
        self.connector_limit = 10  # è¿æ¥æ± å¤§å°ï¼Œæ”¯æŒ5ä¸ªå¹¶å‘ + buffer
        self.connector_limit_per_host = 8  # å•ä¸»æœºæœ€å¤§è¿æ¥æ•°
        self.keepalive_timeout = 30  # ä¿æŒè¿æ¥æ—¶é—´
        
        self.logger.info(f"ğŸš€ TextToVideoGenerator optimized for concurrent tasks")
    
    def _create_optimized_session(self, total_timeout: int = 60) -> aiohttp.ClientSession:
        """åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯ï¼Œæ”¯æŒ5ä¸ªå¹¶å‘è¿æ¥"""
        connector = aiohttp.TCPConnector(
            limit=self.connector_limit,
            limit_per_host=self.connector_limit_per_host,
            keepalive_timeout=self.keepalive_timeout,
            enable_cleanup_closed=True,
            ttl_dns_cache=300,  # DNSç¼“å­˜5åˆ†é’Ÿ
            use_dns_cache=True
        )
        timeout = aiohttp.ClientTimeout(
            total=total_timeout,
            connect=self.connect_timeout,
            sock_read=30
        )
        return aiohttp.ClientSession(connector=connector, timeout=timeout)
    
    def _build_workflow_payload(self, request: TextToVideoRequest) -> Dict[str, Any]:
        """
        æ„å»ºå·¥ä½œæµAPIè½½è· - ä½¿ç”¨RunningHubæ ‡å‡†æ ¼å¼
        
        åŸºäºæ–°å·¥ä½œæµ(1964196221642489858)çš„èŠ‚ç‚¹é…ç½®ï¼š
        - èŠ‚ç‚¹38: æ–‡ç”Ÿå›¾æç¤ºè¯ (image_prompt)
        - èŠ‚ç‚¹10: å›¾ç”Ÿè§†é¢‘æç¤ºè¯ (video_prompt) 
        - èŠ‚ç‚¹1: è´Ÿå‘æç¤ºè¯ (negative_prompt)
        - èŠ‚ç‚¹39: åˆ†è¾¨ç‡è®¾ç½® (width/height)
        - èŠ‚ç‚¹5: WanImageToVideoé•¿åº¦ (length = duration Ã— 16fps)
        - èŠ‚ç‚¹36: éšæœºç§å­ (å¯é€‰)
        """
        
        # RunningHubæ ‡å‡†æ ¼å¼ - ä¸å›¾åƒç”Ÿæˆå™¨æ ¼å¼ä¸€è‡´
        node_list = [
            {
                "nodeId": "38",  # æ–‡ç”Ÿå›¾æç¤ºè¯èŠ‚ç‚¹
                "fieldName": "text",
                "fieldValue": request.image_prompt
            },
            {
                "nodeId": "10",  # å›¾ç”Ÿè§†é¢‘æç¤ºè¯èŠ‚ç‚¹
                "fieldName": "text", 
                "fieldValue": request.video_prompt or request.image_prompt
            },
            {
                "nodeId": "1",   # è´Ÿå‘æç¤ºè¯èŠ‚ç‚¹
                "fieldName": "text",
                "fieldValue": request.negative_prompt or "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
            }
        ]
        
        # æ·»åŠ åˆ†è¾¨ç‡è®¾ç½®
        node_list.extend([
            {
                "nodeId": "39",  # åˆ†è¾¨ç‡èŠ‚ç‚¹
                "fieldName": "width",
                "fieldValue": request.width
            },
            {
                "nodeId": "39",  # åˆ†è¾¨ç‡èŠ‚ç‚¹
                "fieldName": "height",
                "fieldValue": request.height
            }
        ])
        
        # æ·»åŠ WanImageToVideoçš„lengthå‚æ•°ï¼ˆå¸§æ•° = æ—¶é•¿ Ã— 16fpsï¼‰
        wan_frame_count = int(request.duration * 16)  # Wanæ¨¡å‹é»˜è®¤16fps
        node_list.append({
            "nodeId": "5",  # WanImageToVideoèŠ‚ç‚¹
            "fieldName": "length",
            "fieldValue": wan_frame_count
        })
        
        # å¦‚æœæŒ‡å®šäº†seedï¼Œæ·»åŠ éšæœºç§å­æ§åˆ¶
        if request.seed > 0:
            node_list.append({
                "nodeId": "36",  # RandomNoiseèŠ‚ç‚¹
                "fieldName": "noise_seed",
                "fieldValue": request.seed
            })
        
        # RunningHubæ ‡å‡†payloadæ ¼å¼
        payload = {
            "apiKey": self.api_key,
            "workflowId": self.workflow_id,
            "nodeInfoList": node_list
        }
        
        return payload
    
    async def generate_video_async(self, request: TextToVideoRequest, max_task_retries: int = None) -> TextToVideoResult:
        """
        å¼‚æ­¥ç”Ÿæˆæ–‡ç”Ÿè§†é¢‘ï¼ˆæ”¯æŒä»»åŠ¡çº§é‡è¯•ï¼‰
        
        Args:
            request: æ–‡ç”Ÿè§†é¢‘è¯·æ±‚
            max_task_retries: ä»»åŠ¡çº§æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆNoneæ—¶ä»é…ç½®è¯»å–ï¼‰
            
        Returns:
            TextToVideoResult: ç”Ÿæˆç»“æœ
        """
        # ä»é…ç½®è¯»å–é‡è¯•å‚æ•°
        if max_task_retries is None:
            max_task_retries = self.config.get('media.max_retries', 5)
        
        retry_delays = self.config.get('media.retry_delays', [10, 20, 30, 60, 120])
        retry_keywords = self.config.get('media.retry_keywords', ['timeout', 'connection', 'network', 'temporary', 'running', 'server', 'failed'])
        
        overall_start = time.time()
        self.logger.info(f"ğŸ”„ Text-to-video generation with max_retries={max_task_retries}")
        
        # å°è¯•å¤šæ¬¡ç”Ÿæˆï¼ˆä»»åŠ¡çº§é‡è¯•ï¼‰
        for attempt in range(max_task_retries + 1):
            start_time = time.time()
            task_id = None
            
            try:
                if attempt > 0:
                    self.logger.info(f"ğŸ”„ Retry attempt {attempt} for: {request.image_prompt[:50]}...")
                else:
                    self.logger.info(f"ğŸï¸ Starting text-to-video generation: {request.image_prompt[:50]}...")
                
                # æ„å»ºAPIè½½è·
                payload = self._build_workflow_payload(request)
                self.logger.debug(f"Workflow payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
                
                # æäº¤å·¥ä½œæµä»»åŠ¡
                task_id = await self._submit_workflow_task(payload)
                self.logger.info(f"Submitted workflow task: {task_id}")
                
                # è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ
                video_url = await self._poll_task_completion(task_id)
                self.logger.info(f"âœ… Task completed, video URL: {video_url}")
                
                # ä¸‹è½½è§†é¢‘æ–‡ä»¶
                video_path = await self._download_video(video_url, request.scene_id)
                
                # è·å–è§†é¢‘ä¿¡æ¯
                video_info = self._get_video_info(video_path)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                result = TextToVideoResult(
                    video_path=str(video_path),
                    width=video_info.get('width', request.width),
                    height=video_info.get('height', request.height),
                    fps=video_info.get('fps', request.fps),
                    duration=video_info.get('duration', request.duration),
                    file_size=Path(video_path).stat().st_size,
                    provider="runninghub",
                    workflow_id=self.workflow_id,
                    task_id=task_id or "",
                    generation_time=time.time() - overall_start
                )
                
                self.logger.info(f"ğŸ‰ Text-to-video completed: {result.file_size/1024:.1f}KB "
                               f"in {result.generation_time:.2f}s (attempt {attempt + 1})")
                
                return result
                
            except Exception as e:
                attempt_time = time.time() - start_time
                
                # å–æ¶ˆå½“å‰ä»»åŠ¡
                if task_id:
                    try:
                        await self._cancel_task(task_id)
                    except:
                        pass
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < max_task_retries:
                    error_str = str(e).lower()
                    should_retry = any(keyword in error_str for keyword in retry_keywords)
                    
                    if should_retry:
                        # ä½¿ç”¨é…ç½®çš„å»¶è¿Ÿæ—¶é—´ï¼Œå¦‚æœè¶…å‡ºèŒƒå›´åˆ™ä½¿ç”¨æœ€åä¸€ä¸ªå€¼
                        retry_delay = retry_delays[min(attempt, len(retry_delays) - 1)]
                        self.logger.warning(f"â° Attempt {attempt + 1}/{max_task_retries + 1} failed after {attempt_time:.1f}s: {e}")
                        self.logger.info(f"ğŸ”„ Retrying in {retry_delay}s... (delay pattern: {retry_delays})")
                        await asyncio.sleep(retry_delay)
                        continue
                    else:
                        self.logger.error(f"âŒ Non-retryable error on attempt {attempt + 1}: {e}")
                        self.logger.error(f"ğŸ’¡ Error not matching retry keywords: {retry_keywords}")
                        break
                else:
                    self.logger.error(f"âŒ Final attempt {attempt + 1}/{max_task_retries + 1} failed: {e}")
                    break
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
        total_time = time.time() - overall_start
        raise Exception(f"Text-to-video generation failed after {max_task_retries + 1} attempts in {total_time:.1f}s")
    
    async def _submit_workflow_task(self, payload: Dict[str, Any]) -> str:
        """æäº¤å·¥ä½œæµä»»åŠ¡"""
        url = f"{self.base_url}/task/openapi/create"
        headers = {
            "Host": "www.runninghub.cn",
            "Content-Type": "application/json"
        }
        
        async with self._create_optimized_session(60) as session:
            for attempt in range(self.max_retries):
                try:
                    async with session.post(url, json=payload, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            # RunningHub APIæ ¼å¼æ£€æŸ¥
                            if data.get('code') != 0:
                                error_msg = data.get('msg', 'Task creation failed')
                                
                                # ç‰¹æ®Šå¤„ç†é˜Ÿåˆ—æ»¡è½½é”™è¯¯ - ä¸é‡è¯•ï¼Œç›´æ¥å¤±è´¥
                                if 'TASK_QUEUE_MAXED' in error_msg or 'queue' in error_msg.lower():
                                    self.logger.error(f"ğŸš« RunningHub queue is full: {error_msg}")
                                    self.logger.error("ğŸ’¡ Suggestion: Reduce concurrent tasks or wait and retry later")
                                    raise Exception(f"RunningHub queue full (no retry): {error_msg}")
                                
                                raise Exception(f"RunningHub task failed: {error_msg}")
                            
                            # è·å–taskIdï¼ˆæ•´æ•°ç±»å‹ï¼‰
                            task_id = data.get('data', {}).get('taskId')
                            if not task_id:
                                raise ValueError("No taskId in response")
                            return str(task_id)
                        else:
                            error_text = await response.text()
                            raise Exception(f"HTTP {response.status}: {error_text}")
                
                except asyncio.TimeoutError as e:
                    if attempt == self.max_retries - 1:
                        raise Exception(f"Workflow submission timeout after {self.max_retries} attempts")
                    self.logger.warning(f"Workflow submission attempt {attempt + 1} timed out, retrying...")
                    await asyncio.sleep(self.retry_delay)
                
                except Exception as e:
                    error_str = str(e)
                    
                    # é˜Ÿåˆ—æ»¡è½½é”™è¯¯ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡º
                    if 'queue full (no retry)' in error_str or 'TASK_QUEUE_MAXED' in error_str:
                        raise e
                    
                    if attempt == self.max_retries - 1:
                        raise Exception(f"Workflow submission failed: {e}")
                    self.logger.warning(f"Workflow submission attempt {attempt + 1} failed: {e}, retrying...")
                    await asyncio.sleep(self.retry_delay)
    
    async def _poll_task_completion(self, task_id: str) -> str:
        """è½®è¯¢ä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        url = f"{self.base_url}/task/openapi/status"
        headers = {
            "Host": "www.runninghub.cn",
            "Content-Type": "application/json"
        }
        
        poll_interval = 10  # 10ç§’è½®è¯¢é—´éš”
        max_poll_time = self.total_timeout
        start_time = time.time()
        
        status_payload = {"taskId": int(task_id), "apiKey": self.api_key}
        
        self.logger.info(f"â³ Starting to poll task {task_id} (max {max_poll_time}s)")
        
        async with self._create_optimized_session(self.total_timeout) as session:
            poll_count = 0
            while time.time() - start_time < max_poll_time:
                poll_count += 1
                elapsed = time.time() - start_time
                
                try:
                    async with session.post(url, json=status_payload, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            if data.get('code') == 0:
                                task_status = data.get('data')
                                
                                if task_status == 'SUCCESS':
                                    self.logger.info(f"âœ… Task {task_id} SUCCESS after {elapsed:.1f}s ({poll_count} polls)")
                                    
                                    # è·å–ä»»åŠ¡è¾“å‡º
                                    return await self._get_task_outputs(task_id)
                                
                                elif task_status == 'FAILED':
                                    # ä»»åŠ¡åœ¨æœåŠ¡ç«¯å¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                                    self.logger.warning(f"ğŸš« Task {task_id} FAILED on server after {elapsed:.1f}s ({poll_count} polls)")
                                    
                                    # å°è¯•è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
                                    error_details = data.get('msg', 'No error message provided')
                                    full_response = data  # è®°å½•å®Œæ•´å“åº”
                                    self.logger.error(f"ğŸ’¥ Task {task_id} failure details: {error_details}")
                                    self.logger.debug(f"ğŸ” Full API response: {full_response}")
                                    
                                    raise Exception(f"Task {task_id} failed on RunningHub server after {elapsed:.1f}s: {error_details}")
                                
                                elif task_status in ['PENDING', 'RUNNING', 'PROCESSING']:
                                    self.logger.debug(f"ğŸ”„ Task {task_id}: {task_status} (poll #{poll_count}, {elapsed:.1f}s)")
                                
                                else:
                                    self.logger.warning(f"âš ï¸ Task {task_id} unknown status: {task_status}")
                            else:
                                error_msg = data.get('msg', 'Status check failed')
                                self.logger.warning(f"âŒ Task {task_id} status check error: {error_msg}")
                        
                        else:
                            error_text = await response.text()
                            self.logger.warning(f"âŒ Task {task_id} poll failed: HTTP {response.status}: {error_text}")
                
                except asyncio.TimeoutError:
                    self.logger.warning(f"â° Task {task_id} poll timeout (poll #{poll_count}), retrying...")
                
                except Exception as e:
                    # åŒºåˆ†æœåŠ¡ç«¯ä»»åŠ¡å¤±è´¥å’Œç½‘ç»œ/å…¶ä»–é”™è¯¯
                    if "failed on RunningHub server" in str(e):
                        # æœåŠ¡ç«¯ä»»åŠ¡å¤±è´¥ï¼Œåº”è¯¥ç«‹å³ç»ˆæ­¢è½®è¯¢å¹¶é‡æ–°æäº¤ä»»åŠ¡
                        self.logger.error(f"ğŸ’¥ Task {task_id} server failure: {e}")
                        raise e  # é‡æ–°æŠ›å‡ºï¼Œè®©ä¸Šå±‚é‡è¯•æœºåˆ¶å¤„ç†
                    else:
                        # ç½‘ç»œæˆ–å…¶ä»–ä¸´æ—¶é”™è¯¯ï¼Œç»§ç»­è½®è¯¢
                        self.logger.warning(f"âŒ Task {task_id} poll error (poll #{poll_count}): {e}")
                
                # ç­‰å¾…ä¸‹æ¬¡è½®è¯¢
                await asyncio.sleep(poll_interval)
            
            raise Exception(f"Task {task_id} did not complete within {max_poll_time} seconds (made {poll_count} poll attempts)")
    
    async def _get_task_outputs(self, task_id: str) -> str:
        """è·å–ä»»åŠ¡è¾“å‡ºç»“æœ"""
        url = f"{self.base_url}/task/openapi/outputs"
        headers = {
            "Host": "www.runninghub.cn",
            "Content-Type": "application/json"
        }
        
        outputs_payload = {"taskId": int(task_id), "apiKey": self.api_key}
        
        async with self._create_optimized_session(30) as session:
            async with session.post(url, json=outputs_payload, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('code') == 0:
                        outputs = data.get('data', [])
                        
                        # å¯»æ‰¾è§†é¢‘æ–‡ä»¶URL
                        for item in outputs:
                            if isinstance(item, dict) and 'fileUrl' in item:
                                file_url = item['fileUrl']
                                # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                                if file_url.endswith(('.mp4', '.avi', '.mov', '.webm')):
                                    self.logger.info(f"Found video URL: {file_url}")
                                    return file_url
                        
                        # å¦‚æœæ²¡æ‰¾åˆ°è§†é¢‘ï¼Œä½†æœ‰è¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ª
                        if outputs and len(outputs) > 0:
                            if isinstance(outputs[0], dict) and 'fileUrl' in outputs[0]:
                                self.logger.warning("No video file found, using first output")
                                return outputs[0]['fileUrl']
                        
                        raise ValueError("No video output found in task outputs")
                    else:
                        error_msg = data.get('msg', 'Failed to get outputs')
                        raise Exception(f"Get outputs failed: {error_msg}")
                else:
                    error_text = await response.text()
                    raise Exception(f"Failed to get outputs: HTTP {response.status}: {error_text}")
    
    async def _download_video(self, video_url: str, scene_id: str = "") -> Path:
        """ä¸‹è½½ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶"""
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        filename = f"text_to_video_{scene_id or uuid.uuid4().hex[:8]}_{int(time.time())}.mp4"
        output_path = self.file_manager.get_output_path('videos', filename)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with self._create_optimized_session(300) as session:
            async with session.get(video_url) as response:
                if response.status == 200:
                    with open(output_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            f.write(chunk)
                    
                    self.logger.info(f"Downloaded video: {output_path} ({output_path.stat().st_size} bytes)")
                    return output_path
                else:
                    raise Exception(f"Failed to download video: HTTP {response.status}")
    
    def _get_video_info(self, video_path: Path) -> Dict[str, Any]:
        """è·å–è§†é¢‘æ–‡ä»¶ä¿¡æ¯"""
        try:
            import cv2
            
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                return {}
            
            info = {
                'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                'fps': int(cap.get(cv2.CAP_PROP_FPS)),
                'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            }
            info['duration'] = info['frame_count'] / info['fps'] if info['fps'] > 0 else 0
            
            # æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯
            try:
                info['file_size'] = video_path.stat().st_size
            except Exception as size_error:
                self.logger.warning(f"Could not get file size: {size_error}")
                info['file_size'] = 0
            
            cap.release()
            return info
            
        except Exception as e:
            self.logger.warning(f"Could not get video info: {e}")
            return {
                'width': 0,
                'height': 0, 
                'fps': 0,
                'frame_count': 0,
                'duration': 0,
                'file_size': 0
            }
    
    async def _cleanup_failed_task(self, task_id: str):
        """æ¸…ç†å¤±è´¥çš„ä»»åŠ¡èµ„æº"""
        try:
            self.logger.info(f"ğŸ§¹ Cleaning up failed task {task_id}...")
            await self._cancel_task(task_id)
        except Exception as e:
            self.logger.warning(f"Failed to cleanup task {task_id}: {e}")
    
    async def _cancel_task(self, task_id: str):
        """å–æ¶ˆè¿è¡Œä¸­çš„ä»»åŠ¡"""
        try:
            url = f"{self.base_url}/task/openapi/cancel"
            headers = {
                "Host": "www.runninghub.cn",
                "Content-Type": "application/json"
            }
            
            cancel_payload = {"taskId": int(task_id), "apiKey": self.api_key}
            
            async with self._create_optimized_session(30) as session:
                async with session.post(url, json=cancel_payload, headers=headers) as response:
                    if response.status == 200:
                        self.logger.info(f"Successfully cancelled task: {task_id}")
                    else:
                        self.logger.warning(f"Failed to cancel task {task_id}: HTTP {response.status}")
        
        except Exception as e:
            self.logger.warning(f"Error cancelling task {task_id}: {e}")
    
    async def batch_generate_videos(self, requests: List[TextToVideoRequest], 
                                  max_concurrent: int = 3) -> List[TextToVideoResult]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡ç”Ÿè§†é¢‘ - ä¼˜åŒ–ç‰ˆï¼ˆä»»åŠ¡æäº¤ä¸è½®è¯¢åˆ†ç¦»ï¼‰
        
        Args:
            requests: è§†é¢‘ç”Ÿæˆè¯·æ±‚åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            List[TextToVideoResult]: ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        self.logger.info(f"ğŸš€ Starting optimized batch text-to-video generation: {len(requests)} requests (max_concurrent={max_concurrent})")
        
        # ğŸŒ è·å–æ¨¡å—çº§å…¨å±€ä¿¡å·é‡
        actual_concurrent = min(max_concurrent, self._max_concurrent)
        if max_concurrent != actual_concurrent:
            self.logger.warning(f"ğŸ¯ Concurrent limit adjusted: {max_concurrent} â†’ {actual_concurrent} (module global limit)")
        
        # è·å–å…¨å±€ä¿¡å·é‡
        global_semaphore = await get_global_semaphore(actual_concurrent)
        
        # éªŒè¯å¹¶å‘æ•°è®¾ç½®çš„åˆç†æ€§
        if actual_concurrent > 3:
            self.logger.warning(f"âš ï¸  High concurrent setting detected: {actual_concurrent}. RunningHub typically supports max 3 concurrent tasks.")
            self.logger.warning("ğŸ’¡ Consider reducing to 3 or lower if you encounter TASK_QUEUE_MAXED errors.")
        
        # é˜¶æ®µ1ï¼šæ‰¹é‡æäº¤æ‰€æœ‰ä»»åŠ¡
        self.logger.info("ğŸ“ Phase 1: Batch submitting all tasks...")
        
        async def submit_task_with_semaphore(request: TextToVideoRequest) -> tuple:
            async with global_semaphore:  # ğŸŒ ä½¿ç”¨æ¨¡å—çº§å…¨å±€ä¿¡å·é‡
                try:
                    payload = self._build_workflow_payload(request)
                    task_id = await self._submit_workflow_task(payload)
                    self.logger.info(f"âœ… Task submitted: {task_id} for {request.image_prompt[:30]}...")
                    return (request, task_id, None)
                except Exception as e:
                    self.logger.error(f"âŒ Task submission failed for {request.image_prompt[:30]}: {e}")
                    return (request, None, e)
        
        # å¹¶å‘æäº¤æ‰€æœ‰ä»»åŠ¡
        submit_tasks = [submit_task_with_semaphore(req) for req in requests]
        submit_results = await asyncio.gather(*submit_tasks)
        
        # æ”¶é›†æˆåŠŸæäº¤çš„ä»»åŠ¡
        submitted_tasks = [(req, task_id) for req, task_id, error in submit_results if task_id]
        failed_submissions = [(req, error) for req, task_id, error in submit_results if not task_id]
        
        # åˆ†æå¤±è´¥åŸå› 
        queue_full_errors = [error for req, error in failed_submissions if 'queue full' in str(error) or 'TASK_QUEUE_MAXED' in str(error)]
        
        self.logger.info(f"ğŸ“‹ Task submission completed: {len(submitted_tasks)} successful, {len(failed_submissions)} failed")
        
        if queue_full_errors:
            self.logger.error(f"ğŸš« {len(queue_full_errors)} tasks failed due to RunningHub queue being full")
            self.logger.error("ğŸ’¡ Recommendations:")
            self.logger.error("   - Reduce max_concurrent_tasks in config/settings.json")
            self.logger.error(f"   - Current setting: {max_concurrent}, try setting it to 3 or lower")
            self.logger.error("   - Wait a few minutes and retry")
        
        if not submitted_tasks:
            self.logger.error("âŒ No tasks were successfully submitted")
            if queue_full_errors:
                self.logger.error("ğŸ”„ All failures were due to queue being full - try again with lower concurrency")
            return []
        
        # é˜¶æ®µ2ï¼šå¹¶å‘è½®è¯¢æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
        self.logger.info(f"â±ï¸ Phase 2: Polling {len(submitted_tasks)} tasks concurrently...")
        
        async def poll_and_download_with_semaphore(request: TextToVideoRequest, task_id: str) -> TextToVideoResult:
            async with global_semaphore:  # ğŸŒ ä½¿ç”¨æ¨¡å—çº§å…¨å±€ä¿¡å·é‡
                try:
                    start_time = time.time()
                    
                    # è½®è¯¢ä»»åŠ¡å®Œæˆ
                    video_url = await self._poll_task_completion(task_id)
                    
                    # ä¸‹è½½è§†é¢‘
                    video_path = await self._download_video(video_url, request.scene_id)
                    
                    # è·å–è§†é¢‘ä¿¡æ¯
                    video_info = self._get_video_info(video_path)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    result = TextToVideoResult(
                        video_path=str(video_path),
                        width=video_info.get('width', request.width),
                        height=video_info.get('height', request.height),
                        fps=video_info.get('fps', request.fps),
                        duration=video_info.get('duration', request.duration),
                        file_size=Path(video_path).stat().st_size,
                        provider="runninghub",
                        workflow_id=self.workflow_id,
                        task_id=task_id,
                        generation_time=time.time() - start_time
                    )
                    
                    self.logger.info(f"âœ… Task {task_id} completed: {result.file_size/1024:.1f}KB in {result.generation_time:.2f}s")
                    return result
                    
                except Exception as e:
                    self.logger.error(f"âŒ Task {task_id} failed: {e}")
                    # å–æ¶ˆå¤±è´¥çš„ä»»åŠ¡
                    try:
                        await self._cancel_task(task_id)
                    except:
                        pass
                    raise
        
        # å¹¶å‘è½®è¯¢å’Œä¸‹è½½
        poll_tasks = [poll_and_download_with_semaphore(req, task_id) for req, task_id in submitted_tasks]
        poll_results = await asyncio.gather(*poll_tasks, return_exceptions=True)
        
        # å¤„ç†æœ€ç»ˆç»“æœ
        successful_results = []
        failed_count = len(failed_submissions)  # åŒ…å«æäº¤å¤±è´¥çš„
        
        for i, result in enumerate(poll_results):
            if isinstance(result, Exception):
                self.logger.error(f"Task polling/download failed for task {i}: {result}")
                failed_count += 1
            else:
                successful_results.append(result)
        
        self.logger.info(f"ğŸ‰ Optimized batch generation completed: {len(successful_results)} successful, {failed_count} failed")
        return successful_results
    
    async def batch_generate_videos_v2(self, requests: List[TextToVideoRequest], 
                                      max_concurrent: int = 3) -> List[TextToVideoResult]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡ç”Ÿè§†é¢‘ - ä¸€ä½“åŒ–ç‰ˆæœ¬ï¼ˆé¿å…é˜Ÿåˆ—æº¢å‡ºï¼‰
        
        æ¯ä¸ªä»»åŠ¡ä»æäº¤åˆ°å®Œæˆéƒ½åœ¨å•ä¸ªä¿¡å·é‡ä¿æŠ¤ä¸‹è¿›è¡Œï¼Œé¿å…RunningHubé˜Ÿåˆ—å †ç§¯
        
        Args:
            requests: æ–‡ç”Ÿè§†é¢‘è¯·æ±‚åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆå»ºè®®3æˆ–æ›´ä½ï¼‰
            
        Returns:
            List[TextToVideoResult]: ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        self.logger.info(f"ğŸš€ Starting integrated batch text-to-video generation: {len(requests)} requests (max_concurrent={max_concurrent})")
        
        # ğŸŒ è·å–æ¨¡å—çº§å…¨å±€ä¿¡å·é‡
        actual_concurrent = min(max_concurrent, self._max_concurrent)
        if max_concurrent != actual_concurrent:
            self.logger.warning(f"ğŸ¯ Concurrent limit adjusted: {max_concurrent} â†’ {actual_concurrent} (module global limit)")
        
        # è·å–å…¨å±€ä¿¡å·é‡
        global_semaphore = await get_global_semaphore(actual_concurrent)
        
        # éªŒè¯å¹¶å‘æ•°è®¾ç½®çš„åˆç†æ€§
        if actual_concurrent > 3:
            self.logger.warning(f"âš ï¸  High concurrent setting detected: {actual_concurrent}. RunningHub queue limit appears to be 3 or lower.")
            self.logger.warning("ğŸ’¡ Consider reducing to 3 or lower to avoid TASK_QUEUE_MAXED errors.")
        
        # ğŸ”„ ä¸€ä½“åŒ–æµç¨‹ï¼šæ¯ä¸ªä»»åŠ¡ä»æäº¤åˆ°å®Œæˆéƒ½åœ¨ä¿¡å·é‡ä¿æŠ¤ä¸‹è¿›è¡Œï¼Œé¿å…é˜Ÿåˆ—å †ç§¯
        self.logger.info("ğŸ”„ Integrated approach: Each task submitâ†’pollâ†’download in single semaphore session...")
        
        async def process_single_task_complete(request: TextToVideoRequest) -> TextToVideoResult:
            """ä¸€ä½“åŒ–å¤„ç†å•ä¸ªä»»åŠ¡ï¼šæäº¤â†’è½®è¯¢â†’ä¸‹è½½ï¼Œå…¨ç¨‹åœ¨ä¿¡å·é‡ä¿æŠ¤ä¸‹ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
            async with global_semaphore:  # ğŸŒ å…¨ç¨‹ä¿¡å·é‡ä¿æŠ¤ï¼Œé¿å…é˜Ÿåˆ—å †ç§¯
                # ä»é…ç½®è¯»å–é‡è¯•å‚æ•°
                max_task_retries = self.config.get('media.max_retries', 5)
                retry_delays = self.config.get('media.retry_delays', [10, 20, 30, 60, 120])
                overall_start = time.time()
                
                self.logger.debug(f"ğŸ”„ Task retry config: max_retries={max_task_retries}, delays={retry_delays}")
                
                # å°è¯•å¤šæ¬¡ç”Ÿæˆï¼ˆä»»åŠ¡çº§é‡è¯•ï¼‰
                for attempt in range(max_task_retries + 1):
                    task_id = None
                    try:
                        start_time = time.time()
                        
                        if attempt > 0:
                            self.logger.info(f"ğŸ”„ Retry attempt {attempt} for: {request.image_prompt[:30]}...")
                        
                        # Step 1: æäº¤ä»»åŠ¡
                        payload = self._build_workflow_payload(request)
                        task_id = await self._submit_workflow_task(payload)
                        self.logger.info(f"âœ… Task submitted: {task_id} for {request.image_prompt[:30]}...")
                        
                        # Step 2: è½®è¯¢ä»»åŠ¡å®Œæˆ
                        video_url = await self._poll_task_completion(task_id)
                        
                        # Step 3: ä¸‹è½½è§†é¢‘
                        video_path = await self._download_video(video_url, request.scene_id)
                        
                        # Step 4: è·å–è§†é¢‘ä¿¡æ¯
                        video_info = self._get_video_info(video_path)
                        
                        # Step 5: åˆ›å»ºç»“æœå¯¹è±¡
                        result = TextToVideoResult(
                            video_path=str(video_path),
                            width=video_info.get('width', request.width),
                            height=video_info.get('height', request.height),
                            fps=video_info.get('fps', request.fps),
                            duration=video_info.get('duration', request.duration),
                            file_size=video_info.get('file_size', 0),
                            provider="runninghub",
                            workflow_id=self.workflow_id,
                            task_id=task_id,
                            generation_time=time.time() - start_time
                        )
                        
                        self.logger.info(f"âœ… Integrated task completed: {video_path.name} ({result.file_size/1024/1024:.1f}MB, {result.generation_time:.1f}s)")
                        return result  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        
                    except Exception as e:
                        error_msg = f"Task failed for {request.image_prompt[:30]} (attempt {attempt + 1}/{max_task_retries + 1}): {e}"
                        self.logger.error(f"âŒ {error_msg}")
                        
                        # æ¸…ç†èµ„æº
                        if task_id:
                            try:
                                await self._cleanup_failed_task(task_id)
                            except:
                                pass
                        
                        # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼›å¦åˆ™ç»§ç»­é‡è¯•
                        if attempt < max_task_retries:
                            # ä½¿ç”¨é…ç½®çš„å»¶è¿Ÿæ—¶é—´
                            retry_delay = retry_delays[min(attempt, len(retry_delays) - 1)]
                            self.logger.warning(f"â° Batch task attempt {attempt + 1}/{max_task_retries + 1} failed: {e}")
                            self.logger.info(f"ğŸ”„ Retrying in {retry_delay}s... (configured delays: {retry_delays})")
                            await asyncio.sleep(retry_delay)
                        else:
                            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
                            total_time = time.time() - overall_start
                            final_error = f"Task completely failed after {max_task_retries + 1} attempts in {total_time:.1f}s: {e}"
                            raise Exception(final_error)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰ä»»åŠ¡ï¼ˆä¸€ä½“åŒ–æµç¨‹ï¼‰
        self.logger.info(f"âš¡ Processing {len(requests)} tasks with max_concurrent={actual_concurrent}...")
        process_tasks = [process_single_task_complete(req) for req in requests]
        results = await asyncio.gather(*process_tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœå¹¶æ”¶é›†å¤±è´¥ä»»åŠ¡ä¿¡æ¯
        successful_results = []
        failed_tasks = []
        failed_count = 0
        queue_full_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"âŒ Task {i} failed completely: {result}")
                if 'TASK_QUEUE_MAXED' in str(result) or 'queue full' in str(result):
                    queue_full_count += 1
                failed_count += 1
                # è®°å½•å¤±è´¥ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
                failed_tasks.append({
                    'index': i,
                    'request': requests[i],
                    'error': str(result)
                })
            else:
                # æ·»åŠ åŸå§‹ç´¢å¼•ä¿¡æ¯ï¼Œä¾¿äºä¸Šå±‚æ­£ç¡®æ˜ å°„
                result.original_scene_index = i
                successful_results.append(result)
        
        # è¾“å‡ºè¯¦ç»†æ€»ç»“å’Œå»ºè®®
        self.logger.info(f"ğŸ‰ Integrated batch generation completed: {len(successful_results)} successful, {failed_count} failed")
        
        if failed_count > 0:
            self.logger.warning(f"âš ï¸ æ³¨æ„ï¼š{failed_count}ä¸ªä»»åŠ¡å¤±è´¥ï¼Œæœ€ç»ˆè§†é¢‘å¯èƒ½ç¼ºå°‘éƒ¨åˆ†åœºæ™¯")
            
            if queue_full_count > 0:
                self.logger.error(f"ğŸš« {queue_full_count} tasks failed due to RunningHub queue being full")
                self.logger.error("ğŸ’¡ Queue Fullè§£å†³æ–¹æ¡ˆ:")
                self.logger.error(f"   - å½“å‰å¹¶å‘è®¾ç½®: {actual_concurrent}")
                self.logger.error("   - å»ºè®®é™ä½åˆ°2ä¸ªå¹¶å‘: config/settings.json -> max_concurrent_tasks: 2")
                self.logger.error("   - ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•")
            
            # åˆ†æå¤±è´¥åŸå› å¹¶æä¾›å…·ä½“å»ºè®®
            content_failures = [task for task in failed_tasks if 'queue' not in task['error'].lower()]
            if content_failures:
                self.logger.warning(f"ğŸ“ {len(content_failures)}ä¸ªä»»åŠ¡å› å†…å®¹é—®é¢˜å¤±è´¥:")
                for task in content_failures[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    prompt_preview = task['request'].image_prompt[:50] + "..." if len(task['request'].image_prompt) > 50 else task['request'].image_prompt
                    self.logger.warning(f"   - åœºæ™¯{task['index']}: {prompt_preview}")
                
                self.logger.warning("ğŸ’¡ å†…å®¹å¤±è´¥è§£å†³æ–¹æ¡ˆ:")
                self.logger.warning("   - æŸäº›æç¤ºè¯å¯èƒ½è§¦å‘å†…å®¹è¿‡æ»¤")
                self.logger.warning("   - å»ºè®®ç®€åŒ–å¤æ‚çš„æ”¿æ²»ã€æ•æ„Ÿå†…å®¹æè¿°")
                self.logger.warning("   - å¯ä»¥æ‰‹åŠ¨é‡è¯•å¤±è´¥çš„åœºæ™¯")
            
            # æä¾›è¡¥æ•‘æªæ–½å»ºè®®
            success_rate = len(successful_results) / len(requests) * 100
            if success_rate < 80:
                self.logger.error(f"ğŸš¨ æˆåŠŸç‡è¿‡ä½ ({success_rate:.1f}%)ï¼Œå»ºè®®ï¼š")
                self.logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥")
                self.logger.error("   2. é™ä½å¹¶å‘æ•°åˆ°1-2ä¸ª")
                self.logger.error("   3. ç®€åŒ–æç¤ºè¯å†…å®¹")
                self.logger.error("   4. åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ¬¡2-3ä¸ªåœºæ™¯")
            elif success_rate < 100:
                self.logger.warning(f"ğŸ“Š éƒ¨åˆ†æˆåŠŸ ({success_rate:.1f}%)ï¼Œå½±å“ï¼š")
                self.logger.warning("   - æœ€ç»ˆè§†é¢‘ä¼šç¼ºå°‘éƒ¨åˆ†åœºæ™¯")
                self.logger.warning("   - å»ºè®®å•ç‹¬é‡è¯•å¤±è´¥çš„åœºæ™¯")
        
        return successful_results
    
    def generate_video_sync(self, request: TextToVideoRequest) -> TextToVideoResult:
        """åŒæ­¥ç”Ÿæˆæ¥å£"""
        return asyncio.run(self.generate_video_async(request))
    
    def get_supported_resolutions(self) -> List[tuple]:
        """è·å–æ”¯æŒçš„åˆ†è¾¨ç‡åˆ—è¡¨"""
        return [
            (720, 1280),   # ç«–å±æ ‡å‡†
            (1280, 720),   # æ¨ªå±æ ‡å‡†
            (512, 512),    # æ–¹å½¢
            (768, 768),    # æ–¹å½¢é«˜æ¸…
        ]
    
    def __str__(self) -> str:
        return f"TextToVideoGenerator(workflow_id={self.workflow_id}, provider=runninghub)"