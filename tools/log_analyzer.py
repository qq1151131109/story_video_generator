#!/usr/bin/env python3
"""
æ—¥å¿—åˆ†æå·¥å…· - å¿«é€Ÿå®šä½å’Œæ’æŸ¥é—®é¢˜
ä½¿ç”¨æ–°çš„å¢å¼ºå‹æ—¥å¿—ç³»ç»Ÿåˆ†æé¡¹ç›®è¿è¡ŒçŠ¶æ€
"""
import json
import re
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import argparse

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir: str = "output/logs"):
        self.log_dir = Path(log_dir)
        self.log_files = {
            'main': self.log_dir / 'story_generator.log',
            'errors': self.log_dir / 'errors.log',
            'performance': self.log_dir / 'performance.log'
        }
    
    def analyze_errors(self, hours: int = 24) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        print(f"ğŸ” åˆ†ææœ€è¿‘ {hours} å°æ—¶çš„é”™è¯¯...")
        
        errors_file = self.log_files['errors']
        if not errors_file.exists():
            return {"status": "no_errors_file", "message": "é”™è¯¯æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"}
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        error_stats = defaultdict(int)
        error_details = []
        
        try:
            with open(errors_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        log_entry = json.loads(line.strip())
                        
                        # è§£ææ—¶é—´æˆ³
                        timestamp = datetime.fromisoformat(log_entry.get('timestamp', ''))
                        if timestamp < cutoff_time:
                            continue
                        
                        # æå–é”™è¯¯ä¿¡æ¯
                        context = log_entry.get('context', {})
                        error_type = context.get('error_type', 'Unknown')
                        error_message = context.get('error_message', log_entry.get('message', ''))
                        
                        error_key = f"{error_type}: {error_message}"
                        error_stats[error_key] += 1
                        
                        error_details.append({
                            'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                            'type': error_type,
                            'message': error_message,
                            'component': log_entry.get('component', ''),
                            'line': line_num
                        })
                        
                    except (json.JSONDecodeError, ValueError) as e:
                        continue  # è·³è¿‡æ— æ³•è§£æçš„è¡Œ
        
        except Exception as e:
            return {"status": "error", "message": f"è¯»å–é”™è¯¯æ—¥å¿—å¤±è´¥: {e}"}
        
        # æŒ‰å‘ç”Ÿæ¬¡æ•°æ’åº
        top_errors = sorted(error_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "status": "success",
            "total_errors": len(error_details),
            "unique_errors": len(error_stats),
            "top_errors": top_errors,
            "recent_errors": error_details[-5:] if error_details else [],
            "error_trend": self._analyze_error_trend(error_details)
        }
    
    def analyze_performance(self, hours: int = 24) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ—¥å¿—"""
        print(f"âš¡ åˆ†ææœ€è¿‘ {hours} å°æ—¶çš„æ€§èƒ½...")
        
        perf_file = self.log_files['performance']
        if not perf_file.exists():
            return {"status": "no_performance_file", "message": "æ€§èƒ½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"}
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        api_calls = []
        operations = []
        
        try:
            with open(perf_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line.strip())
                        
                        # è§£ææ—¶é—´æˆ³
                        timestamp = datetime.fromisoformat(log_entry.get('timestamp', ''))
                        if timestamp < cutoff_time:
                            continue
                        
                        performance = log_entry.get('performance', {})
                        
                        # APIè°ƒç”¨æ€§èƒ½
                        if 'method' in performance:
                            api_calls.append({
                                'timestamp': timestamp,
                                'method': performance.get('method'),
                                'url': performance.get('url', ''),
                                'status_code': performance.get('status_code'),
                                'response_time': performance.get('response_time'),
                                'success': performance.get('success', True)
                            })
                        
                        # æ“ä½œæ€§èƒ½
                        elif 'duration' in performance:
                            operations.append({
                                'timestamp': timestamp,
                                'duration': performance.get('duration'),
                                'success': performance.get('success', True)
                            })
                            
                    except (json.JSONDecodeError, ValueError):
                        continue
        
        except Exception as e:
            return {"status": "error", "message": f"è¯»å–æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}"}
        
        # åˆ†æAPIæ€§èƒ½
        api_stats = self._analyze_api_performance(api_calls)
        
        # åˆ†ææ“ä½œæ€§èƒ½
        op_stats = self._analyze_operation_performance(operations)
        
        return {
            "status": "success",
            "api_performance": api_stats,
            "operation_performance": op_stats,
            "performance_summary": {
                "total_api_calls": len(api_calls),
                "total_operations": len(operations),
                "avg_api_time": api_stats.get('avg_response_time', 0),
                "slow_operations": len([op for op in operations if op['duration'] > 10.0])
            }
        }
    
    def analyze_system_health(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿå¥åº·çŠ¶æ€"""
        print("ğŸ¥ åˆ†æç³»ç»Ÿå¥åº·çŠ¶æ€...")
        
        health = {
            "status": "healthy",
            "issues": [],
            "warnings": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
        for name, log_file in self.log_files.items():
            if log_file.exists():
                size_mb = log_file.stat().st_size / 1024 / 1024
                if size_mb > 50:  # è¶…è¿‡50MB
                    health["issues"].append(f"{name}æ—¥å¿—æ–‡ä»¶è¿‡å¤§: {size_mb:.1f}MB")
                    health["recommendations"].append(f"æ¸…ç†{name}æ—¥å¿—æˆ–å¢åŠ è½®è½¬é¢‘ç‡")
                elif size_mb > 20:  # è¶…è¿‡20MB
                    health["warnings"].append(f"{name}æ—¥å¿—æ–‡ä»¶è¾ƒå¤§: {size_mb:.1f}MB")
        
        # åˆ†æé”™è¯¯ç‡
        error_analysis = self.analyze_errors(hours=1)
        if error_analysis["status"] == "success" and error_analysis["total_errors"] > 50:
            health["issues"].append(f"è¿‡å»1å°æ—¶é”™è¯¯è¿‡å¤š: {error_analysis['total_errors']}ä¸ª")
            health["status"] = "warning"
        
        # æ£€æŸ¥æ€§èƒ½é—®é¢˜
        perf_analysis = self.analyze_performance(hours=1)
        if perf_analysis["status"] == "success":
            slow_ops = perf_analysis["performance_summary"].get("slow_operations", 0)
            if slow_ops > 5:
                health["warnings"].append(f"æ£€æµ‹åˆ°{slow_ops}ä¸ªæ…¢æ“ä½œ")
        
        # è®¾ç½®æ€»ä½“çŠ¶æ€
        if health["issues"]:
            health["status"] = "unhealthy"
        elif health["warnings"]:
            health["status"] = "warning"
        
        return health
    
    def _analyze_error_trend(self, error_details: List[Dict]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯è¶‹åŠ¿"""
        if not error_details:
            return {"trend": "stable", "message": "æ— é”™è¯¯è®°å½•"}
        
        # æŒ‰å°æ—¶åˆ†ç»„
        hourly_errors = defaultdict(int)
        for error in error_details:
            hour = error['timestamp'][:13]  # YYYY-MM-DD HH
            hourly_errors[hour] += 1
        
        if len(hourly_errors) < 2:
            return {"trend": "insufficient_data", "message": "æ•°æ®ä¸è¶³ä»¥åˆ†æè¶‹åŠ¿"}
        
        # ç®€å•è¶‹åŠ¿åˆ†æ
        recent_hours = sorted(hourly_errors.items())[-3:]  # æœ€è¿‘3å°æ—¶
        if len(recent_hours) >= 2:
            if recent_hours[-1][1] > recent_hours[-2][1] * 1.5:
                return {"trend": "increasing", "message": "é”™è¯¯æ•°é‡å¿«é€Ÿå¢åŠ "}
            elif recent_hours[-1][1] < recent_hours[-2][1] * 0.5:
                return {"trend": "decreasing", "message": "é”™è¯¯æ•°é‡æ˜¾è‘—å‡å°‘"}
        
        return {"trend": "stable", "message": "é”™è¯¯æ•°é‡ç›¸å¯¹ç¨³å®š"}
    
    def _analyze_api_performance(self, api_calls: List[Dict]) -> Dict[str, Any]:
        """åˆ†æAPIæ€§èƒ½"""
        if not api_calls:
            return {"message": "æ— APIè°ƒç”¨è®°å½•"}
        
        # å“åº”æ—¶é—´ç»Ÿè®¡
        response_times = [call['response_time'] for call in api_calls if call['response_time']]
        success_rate = sum(1 for call in api_calls if call['success']) / len(api_calls)
        
        # æŒ‰URLåˆ†ç»„ç»Ÿè®¡
        url_stats = defaultdict(list)
        for call in api_calls:
            if call['response_time']:
                url_stats[call['url']].append(call['response_time'])
        
        # æ‰¾å‡ºæœ€æ…¢çš„API
        slow_apis = []
        for url, times in url_stats.items():
            avg_time = sum(times) / len(times)
            if avg_time > 5.0:  # è¶…è¿‡5ç§’
                slow_apis.append({"url": url, "avg_time": avg_time, "calls": len(times)})
        
        return {
            "total_calls": len(api_calls),
            "success_rate": success_rate,
            "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "slow_apis": sorted(slow_apis, key=lambda x: x['avg_time'], reverse=True)[:5]
        }
    
    def _analyze_operation_performance(self, operations: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ“ä½œæ€§èƒ½"""
        if not operations:
            return {"message": "æ— æ“ä½œè®°å½•"}
        
        durations = [op['duration'] for op in operations]
        success_rate = sum(1 for op in operations if op['success']) / len(operations)
        
        return {
            "total_operations": len(operations),
            "success_rate": success_rate,
            "avg_duration": sum(durations) / len(durations),
            "max_duration": max(durations),
            "slow_operations": len([d for d in durations if d > 10.0])
        }
    
    def generate_report(self, hours: int = 24) -> str:
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        print(f"ğŸ“Š ç”Ÿæˆæœ€è¿‘ {hours} å°æ—¶çš„å®Œæ•´æŠ¥å‘Š...")
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        error_analysis = self.analyze_errors(hours)
        perf_analysis = self.analyze_performance(hours)
        health_analysis = self.analyze_system_health()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 60)
        report.append(f"ğŸ“Š æ•…äº‹è§†é¢‘ç”Ÿæˆå™¨ - æ—¥å¿—åˆ†ææŠ¥å‘Š")
        report.append(f"ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶")
        report.append(f"ğŸ• ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        
        # ç³»ç»Ÿå¥åº·çŠ¶æ€
        report.append(f"\nğŸ¥ ç³»ç»Ÿå¥åº·çŠ¶æ€: {health_analysis['status'].upper()}")
        if health_analysis['issues']:
            report.append("âŒ å‘ç°çš„é—®é¢˜:")
            for issue in health_analysis['issues']:
                report.append(f"   â€¢ {issue}")
        
        if health_analysis['warnings']:
            report.append("âš ï¸  è­¦å‘Š:")
            for warning in health_analysis['warnings']:
                report.append(f"   â€¢ {warning}")
        
        if health_analysis['recommendations']:
            report.append("ğŸ’¡ å»ºè®®:")
            for rec in health_analysis['recommendations']:
                report.append(f"   â€¢ {rec}")
        
        # é”™è¯¯åˆ†æ
        report.append(f"\nğŸš¨ é”™è¯¯åˆ†æ:")
        if error_analysis['status'] == 'success':
            report.append(f"   æ€»é”™è¯¯æ•°: {error_analysis['total_errors']}")
            report.append(f"   ç‹¬ç‰¹é”™è¯¯: {error_analysis['unique_errors']}")
            report.append(f"   é”™è¯¯è¶‹åŠ¿: {error_analysis['error_trend']['message']}")
            
            if error_analysis['top_errors']:
                report.append("   ğŸ”¥ é«˜é¢‘é”™è¯¯:")
                for error, count in error_analysis['top_errors'][:5]:
                    report.append(f"      â€¢ {error}: {count}æ¬¡")
        else:
            report.append(f"   çŠ¶æ€: {error_analysis['message']}")
        
        # æ€§èƒ½åˆ†æ
        report.append(f"\nâš¡ æ€§èƒ½åˆ†æ:")
        if perf_analysis['status'] == 'success':
            summary = perf_analysis['performance_summary']
            report.append(f"   APIè°ƒç”¨æ•°: {summary['total_api_calls']}")
            report.append(f"   æ“ä½œæ•°: {summary['total_operations']}")
            report.append(f"   å¹³å‡APIå“åº”æ—¶é—´: {summary['avg_api_time']:.2f}s")
            report.append(f"   æ…¢æ“ä½œæ•°: {summary['slow_operations']}")
            
            # APIæ€§èƒ½è¯¦æƒ…
            api_perf = perf_analysis['api_performance']
            if 'success_rate' in api_perf:
                report.append(f"   APIæˆåŠŸç‡: {api_perf['success_rate']*100:.1f}%")
                
                if api_perf.get('slow_apis'):
                    report.append("   ğŸŒ æ…¢API:")
                    for api in api_perf['slow_apis'][:3]:
                        report.append(f"      â€¢ {api['url']}: {api['avg_time']:.2f}s ({api['calls']}æ¬¡è°ƒç”¨)")
        else:
            report.append(f"   çŠ¶æ€: {perf_analysis['message']}")
        
        report.append("\n" + "=" * 60)
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥å¿—åˆ†æå·¥å…· - å¿«é€Ÿå®šä½é—®é¢˜")
    parser.add_argument('--hours', '-t', type=int, default=24, help='åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')
    parser.add_argument('--errors', '-e', action='store_true', help='åªåˆ†æé”™è¯¯')
    parser.add_argument('--performance', '-p', action='store_true', help='åªåˆ†ææ€§èƒ½')
    parser.add_argument('--health', action='store_true', help='åªæ£€æŸ¥ç³»ç»Ÿå¥åº·')
    parser.add_argument('--log-dir', '-d', default='output/logs', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    
    if args.errors:
        result = analyzer.analyze_errors(args.hours)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    elif args.performance:
        result = analyzer.analyze_performance(args.hours)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    elif args.health:
        result = analyzer.analyze_system_health()
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report = analyzer.generate_report(args.hours)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = Path(args.log_dir) / f'analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

if __name__ == "__main__":
    main()